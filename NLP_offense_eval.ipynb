{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "NLP - offense eval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ollema/nlp_offenseeval/blob/master/NLP_offense_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSd4_idAEImt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "bfbc4ec5-2412-45f0-a509-0b4fdbc8fe65"
      },
      "source": [
        "!pip install pytorch-crf\n",
        "!pip install transformers\n",
        "!pip install wordsegment\n",
        "!pip install emoji\n",
        "!pip install tdqm\n",
        "\n",
        "!wget -c https://raw.githubusercontent.com/ollema/nlp_offenseeval/master/OLIDv1.0/olid-training-v1.0.tsv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.6/dist-packages (0.7.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.35)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.9)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.36 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.36)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: wordsegment in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "--2019-12-17 13:44:23--  https://raw.githubusercontent.com/ollema/nlp_offenseeval/master/OLIDv1.0/olid-training-v1.0.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHgz_RkmCEuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import torch\n",
        "from torch import nn\n",
        "import time\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "import random\n",
        "\n",
        "import emoji\n",
        "from wordsegment import load, segment\n",
        "load()\n",
        "\n",
        "from transformers import DistilBertTokenizer as BertTokenizer\n",
        "from transformers import DistilBertForSequenceClassification as BertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina' \n",
        "plt.style.use('seaborn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO4i3rrqCEuY",
        "colab_type": "text"
      },
      "source": [
        "## 1. Reading the data\n",
        "\n",
        "The following function reads a file with OLID instances.\n",
        "\n",
        "Each instance contains up to 3 labels each.\n",
        "\n",
        "Here is some example instances:\n",
        "\n",
        "```\n",
        "id\ttweet\tsubtask_a\tsubtask_b\tsubtask_c\n",
        "86426\t@USER She should ask a few native Americans what their take on this is.\tOFF\tUNT\tNULL\n",
        "90194\t@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL\tOFF\tTIN\tIND\n",
        "16820\tAmazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT\tNOT\tNULL\tNULL\n",
        "62688\t\"@USER Someone should'veTaken\"\" this piece of shit to a volcano. ðŸ˜‚\"\"\"\tOFF\tUNT\tNULL\n",
        "43605\t@USER @USER Obama wanted liberals &amp; illegals to move into red states\tNOT\tNULL\tNULL\n",
        "```\n",
        "Instances are included in TSV format as follows:\n",
        "\n",
        "`ID\tINSTANCE\tSUBA\tSUBB\tSUBC `\n",
        "\n",
        "Whenever a label is not given, a value `NULL` is inserted (e.g. `INSTANCE\tNOT\tNULL\tNULL`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_KRR3BtCEua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def desegmentize_hashtags_in_tweet(tweet):\n",
        "    new_tweet = []\n",
        "    for word in tweet.split():\n",
        "        new_word = word\n",
        "        if word[0] == '#':\n",
        "            new_word = \" \".join(segment(word[1:]))\n",
        "        new_tweet.append(new_word)\n",
        "    return \" \".join(new_tweet)\n",
        "\n",
        "def limit_users_in_tweet(tweet):\n",
        "    new_tweet = []\n",
        "    user_count = 0\n",
        "    for word in tweet.split():\n",
        "        if word == \"@USER\":\n",
        "            user_count += 1\n",
        "        else:\n",
        "            user_count = 0\n",
        "        if user_count <= 3:\n",
        "            new_tweet.append(word)\n",
        "    return \" \".join(new_tweet)\n",
        "\n",
        "def read_data(corpus_file, datafields, tokenizer, max_len):\n",
        "    print(f'Reading sentences from {corpus_file}...')\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    with open(corpus_file, encoding='utf-8') as f:\n",
        "        next(f) # skip header line\n",
        "        \n",
        "        n_truncated = 0\n",
        "        examples = []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            _, tweet, label, _, _ = line.split(\"\\t\")\n",
        "\n",
        "            # desegmentize hashtags in tweet\n",
        "            tweet = desegmentize_hashtags_in_tweet(tweet)\n",
        "\n",
        "            # demojize tweet\n",
        "            tweet = emoji.demojize(tweet).replace(\":\", \" \").replace(\"_\", \" \")\n",
        "\n",
        "            # replace URL with http\n",
        "            tweet = tweet.replace(\"URL\", \"http\")\n",
        "\n",
        "            # limit the amount of consecutive @USERs in a tweet\n",
        "            if tweet.count(\"@USER\") > 3:\n",
        "                tweet = limit_users_in_tweet(tweet)\n",
        "\n",
        "            tokens = tokenizer.tokenize(tweet)\n",
        "            \n",
        "            # we need to truncate the sentences\n",
        "            if len(tokens) > max_len-2:\n",
        "                tokens = tokens[:max_len-2]\n",
        "                n_truncated += 1\n",
        "\n",
        "            tweet = \" \".join(tokens)\n",
        "            examples.append(torchtext.data.Example.fromlist([tweet, label], datafields))\n",
        "        \n",
        "        print(f'Read {len(examples)} sentences, truncated {n_truncated}.')\n",
        "        return torchtext.data.Dataset(examples, datafields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnwFBnlMCEvD",
        "colab_type": "text"
      },
      "source": [
        "## 2. Training the classifier\n",
        "Note that the `train` method returns the best F1-score seen when evaluating on the validation set.\n",
        "\n",
        "The `classify` method will be used in the interactive demo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IczR7AqyCEvF",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_validation(scores, gold):\n",
        "    guesses = scores.argmax(dim=1)\n",
        "    return (guesses == gold).sum().item()\n",
        "\n",
        "\n",
        "class Classifier:\n",
        "    def __init__(self, config, gensim_model=None, bert_model_name=None):\n",
        "        self.config = config\n",
        "        self.bert_model_name = bert_model_name\n",
        "        lowercase = 'uncased' in bert_model_name\n",
        "        print('Lowercased BERT model?', lowercase)\n",
        "        \n",
        "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=lowercase)\n",
        "        pad = self.tokenizer.pad_token           \n",
        "        # self.WORD = torchtext.data.Field(init_token=self.tokenizer.cls_token, eos_token=self.tokenizer.sep_token, sequential=True, lower=lowercase, pad_token=pad, batch_first=True)\n",
        "        # self.LABEL = torchtext.data.Field(is_target=True, init_token='O', eos_token=pad, pad_token=pad, sequential=True, unk_token=None, batch_first=True)\n",
        "\n",
        "        self.WORD = torchtext.data.Field(sequential=True, tokenize=self.tokenizer.tokenize, pad_token=self.tokenizer.pad_token, init_token=self.tokenizer.cls_token, eos_token=self.tokenizer.sep_token)\n",
        "        self.LABEL = torchtext.data.LabelField(is_target=True)\n",
        "\n",
        "        self.fields = [('tweet', self.WORD), ('label', self.LABEL)]     \n",
        "        self.device = 'cuda'\n",
        "                \n",
        "    def train(self):\n",
        "        print('Reading and tokenizing...')\n",
        "        dataset = read_data(self.config.dataset, self.fields, self.tokenizer, 128) \n",
        "        train, valid = dataset.split([0.8, 0.2])\n",
        "\n",
        "        self.LABEL.build_vocab(train)\n",
        "        self.WORD.build_vocab(train)\n",
        "        # Here, we tell torchtext to use the vocabulary of BERT's tokenizer.\n",
        "        # .stoi is the map from strings to integers, and itos from integers to strings.\n",
        "        self.WORD.vocab.stoi = self.tokenizer.vocab\n",
        "        self.WORD.vocab.itos = list(self.tokenizer.vocab)\n",
        "\n",
        "        print(f\"Using BertForSequenceClassification\")\n",
        "        self.model = BertForSequenceClassification.from_pretrained(self.bert_model_name, num_labels=2)\n",
        "        self.model.to(self.device)\n",
        "            \n",
        "        train_iterator = torchtext.data.BucketIterator(\n",
        "            train,\n",
        "            device=self.device,\n",
        "            batch_size=self.config.train_batch_size,\n",
        "            sort_key=lambda x: len(x.tweet),\n",
        "            repeat=False,\n",
        "            train=True,\n",
        "            sort=True)\n",
        "\n",
        "        valid_iterator = torchtext.data.BucketIterator(\n",
        "            valid,\n",
        "            device=self.device,\n",
        "            batch_size=self.config.valid_batch_size,\n",
        "            sort_key=lambda x: len(x.tweet),\n",
        "            repeat=False,\n",
        "            train=False,\n",
        "            sort=True)\n",
        "        \n",
        "        # train_iterator = torchtext.data.Iterator(\n",
        "        #     train,\n",
        "        #     device=device,\n",
        "        #     batch_size=32,\n",
        "        #     repeat=False,\n",
        "        #     train=True,\n",
        "        #     sort=False)\n",
        "\n",
        "        # valid_iterator = torchtext.data.Iterator(\n",
        "        #     valid,\n",
        "        #     device=device,\n",
        "        #     batch_size=32,\n",
        "        #     repeat=False,\n",
        "        #     train=False,\n",
        "        #     sort=False)\n",
        "        \n",
        "        train_batches = list(train_iterator)\n",
        "        valid_batches = list(valid_iterator)\n",
        "        \n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        decay = 0.01\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': decay},\n",
        "            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "            \n",
        "        # As discussed above, we use the AdamW optimizer from the transformers library. It seems to\n",
        "        # give slightly better results than the standard Adam.\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8)\n",
        "        \n",
        "        history = defaultdict(list)    \n",
        "        best_f1 = -1\n",
        "        \n",
        "        for epoch in range(1, self.config.n_epochs + 1):\n",
        "\n",
        "            t0 = time.time()\n",
        "        \n",
        "            loss_sum = 0\n",
        "            n_batches = 0\n",
        "\n",
        "            self.model.train()\n",
        "            \n",
        "            print('Training')\n",
        "            sys.stdout.flush()\n",
        "\n",
        "            for i, batch in enumerate(tqdm(train_iterator)):\n",
        "\n",
        "                tweets = batch.tweet.t()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = self.model(tweets, labels=batch.label)\n",
        "                            \n",
        "                loss = outputs[0]\n",
        "                \n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                            \n",
        "                loss_sum += loss.item()\n",
        "                n_batches += 1\n",
        "            \n",
        "            train_loss = loss_sum / n_batches\n",
        "            history['train_loss'].append(train_loss)\n",
        "            \n",
        "            n_correct = 0\n",
        "            n_valid = len(valid)\n",
        "            loss_sum = 0\n",
        "            n_batches = 0\n",
        "\n",
        "            self.model.eval()\n",
        "\n",
        "            print('\\nValidating', end='')\n",
        "            sys.stdout.flush()\n",
        "            \n",
        "            for i, batch in enumerate(tqdm(valid_iterator)):\n",
        "                tweets = batch.tweet.t()\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    outputs = self.model(tweets, labels=batch.label)\n",
        "                    loss_batch, scores = outputs\n",
        "                    \n",
        "                loss_sum += loss_batch.item()\n",
        "                n_correct += evaluate_validation(scores, batch.label)\n",
        "                n_batches += 1\n",
        "\n",
        "            val_acc = n_correct / n_valid\n",
        "            val_loss = loss_sum / n_batches\n",
        "\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['val_acc'].append(val_acc)   \n",
        "                    \n",
        "            t1 = time.time()\n",
        "            print()\n",
        "            print(f'Epoch {epoch}: train loss = {train_loss:.4f}, val loss = {val_loss:.4f}, val acc: {val_acc:.4f}, time = {t1-t0:.4f}')\n",
        "        \n",
        "        return history[\"val_acc\"][-1]\n",
        "        \n",
        "    # def tag(self, sentences):\n",
        "    #     \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9QksBhnCEvK",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "class ClassifierConfig(object):\n",
        "    \n",
        "    # Location of training and validation data.\n",
        "    dataset = 'olid-training-v1.0.tsv'\n",
        "    \n",
        "    # Batch size for the training and validation set.\n",
        "    train_batch_size = 32\n",
        "    valid_batch_size = 32\n",
        "    \n",
        "    # Number of training epochs.\n",
        "    n_epochs=1\n",
        "    \n",
        "    # Word dropout probability.\n",
        "    word_dropout_prob = 0.2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTYRNJBMCEvO",
        "colab_type": "code",
        "outputId": "12ce7e83-c6b9-4035-e3c2-bb94df3336f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "source": [
        "f_scores = []\n",
        "\n",
        "for i in range(1):\n",
        "    torch.manual_seed(i * 1000) and random.seed(i * 1000)\n",
        "\n",
        "    classifier = Classifier(config=ClassifierConfig(), bert_model_name=\"distilbert-base-uncased\")\n",
        "\n",
        "    f_scores.append(classifier.train())\n",
        "\n",
        "print(f\"mean f-score: {np.mean(f_scores)}\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lowercased BERT model? True\n",
            "Reading and tokenizing...\n",
            "Reading sentences from olid-training-v1.0.tsv...\n",
            "Read 13240 sentences, truncated 1.\n",
            "Using BertForSequenceClassification\n",
            "Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 331/331 [01:25<00:00,  1.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validating"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83/83 [00:06<00:00,  4.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: train loss = 0.4771, val loss = 0.4447, val acc: 0.7893, time = 91.8168\n",
            "mean f-score: 0.7892749244712991\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}