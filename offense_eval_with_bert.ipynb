{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "offense eval with bert",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPu/g6fQKvhKzI6zn79zhx6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ollema/nlp_offenseeval/blob/master/offense_eval_with_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sba5DSRYTvYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/ollema/nlp_offenseeval/master/OLIDv1.0/olid-training-v1.0.tsv -O training.tsv\n",
        "# !wget https://raw.githubusercontent.com/ollema/nlp_offenseeval/master/OLIDv1.0/offenseval-trial.txt -O trial.tsv\n",
        "!wget https://raw.githubusercontent.com/ollema/nlp_offenseeval/master/OLIDv1.0/test_set_stiched.tsv -O test.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQuxnM2LUexN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install pytorch-pretrained-bert\n",
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFlMy-DxAC6J",
        "colab_type": "text"
      },
      "source": [
        "# Model definition and data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-__iJ5A3uDpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import csv\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from pytorch_pretrained_bert.optimization import BertAdam\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
        "\n",
        "\n",
        "class InputExample(object):\n",
        "    def __init__(self, guid, text, label=None):\n",
        "        self.guid = guid\n",
        "        self.text = text\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    def __init__(self, input_ids, attention_mask, label):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "    def get_train_examples(self, training_data):\n",
        "        return self._create_trn_examples(self._read_tsv(training_data))\n",
        "\n",
        "    def get_eval_examples(self, eval_data):\n",
        "        return self._create_eval_examples(self._read_tsv(eval_data))\n",
        "\n",
        "    def get_labels(self):\n",
        "        return [\"0\", \"1\"]\n",
        "\n",
        "    def _create_trn_examples(self, lines):\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if i != 0:\n",
        "                guid = line[0]\n",
        "                text = line[1]\n",
        "                if line[2] == \"OFF\":\n",
        "                    label = \"1\"\n",
        "                else:\n",
        "                    label = \"0\"\n",
        "                examples.append(InputExample(guid=guid, text=text, label=label))\n",
        "        return examples\n",
        "\n",
        "    def _create_eval_examples(self, lines):\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if i != 0:\n",
        "                guid = line[0]\n",
        "                text = line[1]\n",
        "                if line[2] == \"OFF\":\n",
        "                    label = \"1\"\n",
        "                else:\n",
        "                    label = \"0\"\n",
        "                examples.append(InputExample(guid=guid, text=text, label=label))\n",
        "        return examples\n",
        "\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
        "    label_map = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        tokens = tokenizer.tokenize(example.text)\n",
        "\n",
        "        # Account for [CLS] and [SEP] with \"- 2\"\n",
        "        if len(tokens) > max_seq_length - 2:\n",
        "            tokens = tokens[: (max_seq_length - 2)]\n",
        "\n",
        "        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        attention_mask += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(attention_mask) == max_seq_length\n",
        "\n",
        "        label = label_map[example.label]\n",
        "        if ex_index < 0:\n",
        "            print(\"\\n*** Example ***\")\n",
        "            print(\"guid: %s\" % (example.guid))\n",
        "            print(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
        "            print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            print(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
        "            print(\"label: %s (id = %d)\" % (example.label, label))\n",
        "\n",
        "        features.append(InputFeatures(input_ids=input_ids, attention_mask=attention_mask, label=label))\n",
        "    return features\n",
        "\n",
        "\n",
        "def accuracy(out, gold):\n",
        "    guess = np.argmax(out, axis=1)\n",
        "    return (guess == gold).sum()\n",
        "\n",
        "\n",
        "def stats(out, gold):\n",
        "    guess = np.argmax(out, axis=1)\n",
        "\n",
        "    tp = (gold * guess).sum()\n",
        "    tn = ((1 - gold) * (1 - guess)).sum()\n",
        "    fp = ((1 - gold) * guess).sum()\n",
        "    fn = (gold * (1 - guess)).sum()\n",
        "\n",
        "    return tp, tn, fp, fn\n",
        "\n",
        "\n",
        "def warmup_linear(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x / warmup\n",
        "    return 1.0 - x\n",
        "\n",
        "\n",
        "def main(\n",
        "    bert_model=\"bert-base-uncased\",\n",
        "    distil=False,\n",
        "    max_seq_length=80,\n",
        "    do_train=True,\n",
        "    training_data=\"training.tsv\",\n",
        "    do_eval=True,\n",
        "    eval_data=\"test.tsv\",\n",
        "    lower_case=True,\n",
        "    train_batch_size=32,\n",
        "    eval_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=2.0,\n",
        "    warmup_proportion=0.1,\n",
        "    seed=42,\n",
        "):\n",
        "\n",
        "    output_dir = \"./output/\"\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    print(f\"device: {device}\")\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    if os.path.exists(output_dir) and os.listdir(output_dir) and do_train:\n",
        "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(output_dir))\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    processor = DataProcessor()\n",
        "    num_labels = 2\n",
        "    label_list = processor.get_labels()\n",
        "\n",
        "    train_examples = None\n",
        "    num_train_steps = None\n",
        "    if do_train:\n",
        "        train_examples = processor.get_train_examples(training_data)\n",
        "        num_train_steps = int(len(train_examples) / train_batch_size * num_train_epochs)\n",
        "\n",
        "    # Prepare model\n",
        "    if distil:\n",
        "        bert_model = \"distil\" + bert_model\n",
        "        tokenizer = DistilBertTokenizer.from_pretrained(bert_model, do_lower_case=lower_case)\n",
        "        model = DistilBertForSequenceClassification.from_pretrained(bert_model, num_labels=num_labels)\n",
        "\n",
        "    else:\n",
        "        tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=lower_case)\n",
        "        model = BertForSequenceClassification.from_pretrained(bert_model, num_labels=num_labels)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    global_step = 0\n",
        "    nb_tr_steps = 0\n",
        "    tr_loss = 0\n",
        "    if do_train:\n",
        "        # Prepare optimizer\n",
        "        param_optimizer = list(model.named_parameters())\n",
        "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "            {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        t_total = num_train_steps\n",
        "        optimizer = BertAdam(optimizer_grouped_parameters, lr=learning_rate, warmup=warmup_proportion, t_total=t_total)\n",
        "\n",
        "        train_features = convert_examples_to_features(train_examples, label_list, max_seq_length, tokenizer)\n",
        "        print(\"\\n\\n***** Running training *****\")\n",
        "        print(f\"  Num examples = {len(train_examples)}\")\n",
        "        print(f\"  Batch size = {train_batch_size}\")\n",
        "        print(f\"  Num steps = {num_train_steps}\")\n",
        "        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "        all_attention_mask = torch.tensor([f.attention_mask for f in train_features], dtype=torch.long)\n",
        "        all_labels = torch.tensor([f.label for f in train_features], dtype=torch.long)\n",
        "        train_data = TensorDataset(all_input_ids, all_attention_mask, all_labels)\n",
        "        train_sampler = RandomSampler(train_data)\n",
        "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(int(num_train_epochs)):\n",
        "            print(f\"\\nepoch {epoch + 1} of {int(num_train_epochs)}:\", flush=True)\n",
        "            tr_loss = 0\n",
        "            nb_tr_examples, nb_tr_steps = 0, 0\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                sys.stdout.write(\"\\r\")\n",
        "                sys.stdout.write(f\"iteration {step:03} of {int(t_total / num_train_epochs):03}\")\n",
        "                sys.stdout.flush()\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                input_ids, attention_mask, labels = batch\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs[0]\n",
        "                loss.backward()\n",
        "\n",
        "                tr_loss += loss.item()\n",
        "                nb_tr_examples += input_ids.size(0)\n",
        "                nb_tr_steps += 1\n",
        "\n",
        "                # modify learning rate with special warm up BERT uses\n",
        "                lr_this_step = learning_rate * warmup_linear(global_step / t_total, warmup_proportion)\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group[\"lr\"] = lr_this_step\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "    # Save a trained model\n",
        "    model_to_save = model.module if hasattr(model, \"module\") else model  # Only save the model it-self\n",
        "    output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
        "    if do_train:\n",
        "        torch.save(model_to_save.state_dict(), output_model_file)\n",
        "\n",
        "    # Load a trained model\n",
        "    model_state_dict = torch.load(output_model_file)\n",
        "    if distil:\n",
        "        model = DistilBertForSequenceClassification.from_pretrained(bert_model, state_dict=model_state_dict, num_labels=num_labels)\n",
        "    else:\n",
        "        model = BertForSequenceClassification.from_pretrained(bert_model, state_dict=model_state_dict, num_labels=num_labels)\n",
        "    model.to(device)\n",
        "\n",
        "    if do_eval:\n",
        "        eval_examples = processor.get_eval_examples(eval_data)\n",
        "        eval_features = convert_examples_to_features(eval_examples, label_list, max_seq_length, tokenizer)\n",
        "        print(\"\\n\\n\\n***** Running evaluation *****\")\n",
        "        print(f\"  Num examples = {len(eval_examples)}\")\n",
        "        print(f\"  Batch size = {eval_batch_size}\\n\")\n",
        "        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
        "        all_attention_mask = torch.tensor([f.attention_mask for f in eval_features], dtype=torch.long)\n",
        "        all_labels = torch.tensor([f.label for f in eval_features], dtype=torch.long)\n",
        "        eval_data = TensorDataset(all_input_ids, all_attention_mask, all_labels)\n",
        "        eval_sampler = SequentialSampler(eval_data)\n",
        "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
        "\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy, = 0, 0\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\n",
        "        nb_total_eval_steps = int(np.ceil(len(eval_examples) / eval_batch_size))\n",
        "        eval_tp, eval_tn, eval_fp, eval_fn = 0, 0, 0, 0\n",
        "\n",
        "        for input_ids, attention_mask, labels in eval_dataloader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            labels = labels.to(\"cpu\").numpy()\n",
        "            tmp_eval_accuracy = accuracy(logits, labels)\n",
        "            tmp_eval_tp, tmp_eval_tn, tmp_eval_fp, tmp_eval_fn = stats(logits, labels)\n",
        "            eval_tp += tmp_eval_tp\n",
        "            eval_tn += tmp_eval_tn\n",
        "            eval_fp += tmp_eval_fp\n",
        "            eval_fn += tmp_eval_fn\n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "            nb_eval_examples += input_ids.size(0)\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "            sys.stdout.write(\"\\r\")\n",
        "            sys.stdout.write(f\"iteration {nb_eval_steps:03} of {nb_total_eval_steps:03}\")\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        eval_loss = eval_loss / nb_eval_steps\n",
        "        eval_accuracy = eval_accuracy / nb_eval_examples\n",
        "        loss = tr_loss / nb_tr_steps if do_train else None\n",
        "\n",
        "        epsilon = 1e-7\n",
        "\n",
        "        offensive_precision = eval_tp / (eval_tp + eval_fp + epsilon)\n",
        "        offensive_recall = eval_tp / (eval_tp + eval_fn + epsilon)\n",
        "\n",
        "        not_offensive_precision = eval_tn / (eval_tn + eval_fn + epsilon)\n",
        "        not_offensive_recall = eval_tn / (eval_tn + eval_fp + epsilon)\n",
        "\n",
        "        offensive_f1 = 2 * (offensive_precision * offensive_recall) / (offensive_precision + offensive_recall + epsilon)\n",
        "        not_offensive_f1 = 2 * (not_offensive_precision * not_offensive_recall) / (not_offensive_precision + not_offensive_recall + epsilon)\n",
        "        f1 = (offensive_f1 + not_offensive_f1) / 2\n",
        "\n",
        "        result = {\"eval_loss\": eval_loss, \"eval_accuracy\": eval_accuracy, \"global_step\": global_step, \"loss\": loss, \"f1_score\": f1}\n",
        "\n",
        "        output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"w\") as writer:\n",
        "            print(\"\\n\\n***** Results *****\")\n",
        "            for key in sorted(result.keys()):\n",
        "                print(f\"  {key} = {result[key]}\")\n",
        "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91TONFcKANVy",
        "colab_type": "text"
      },
      "source": [
        "# Running the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf-ZMxHsPOuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DTgORE9K1zm",
        "colab_type": "code",
        "outputId": "ace2a736-2587-4cba-a58c-fe6c3ac91848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "source": [
        "main(distil=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cuda\n",
            "\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 13240\n",
            "  Batch size = 32\n",
            "  Num steps = 827\n",
            "\n",
            "epoch 1 of 2:\n",
            "iteration 413 of 413\n",
            "epoch 2 of 2:\n",
            "iteration 413 of 413\n",
            "\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = 860\n",
            "  Batch size = 8\n",
            "\n",
            "iteration 108 of 108\n",
            "\n",
            "***** Results *****\n",
            "  eval_accuracy = 0.858139534883721\n",
            "  eval_loss = 0.3630906459358003\n",
            "  f1_score = 0.8137073363597301\n",
            "  global_step = 828\n",
            "  loss = 0.35275051260484014\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}