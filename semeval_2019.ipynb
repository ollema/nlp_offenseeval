{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semeval 2019",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMJyDgkDHfQJz/BQuZmRuFl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ollema/nlp_offenseeval/blob/master/semeval_2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sba5DSRYTvYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/ollema/nlp_offenseeval/master/OLIDv1.0/olid-training-v1.0.tsv -O training.tsv\n",
        "# !wget https://raw.githubusercontent.com/ollema/nlp_offenseeval/master/OLIDv1.0/offenseval-trial.txt -O trial.tsv\n",
        "!wget https://raw.githubusercontent.com/ollema/nlp_offenseeval/master/OLIDv1.0/test_set_stiched.tsv -O test.tsv\n",
        "\n",
        "# spellcorrected versions\n",
        "!wget https://raw.githubusercontent.com/ollema/nlp_offenseeval/master/OLIDv1.0/olid-training_SPELLCORRECTED.tsv -O training_spellcorr.tsv\n",
        "!wget https://raw.githubusercontent.com/ollema/nlp_offenseeval/master/OLIDv1.0/test_set_stiched_SPELLCORRECTED.tsv -O test_spellcorr.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQuxnM2LUexN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install pytorch-pretrained-bert\n",
        "!pip install transformers\n",
        "!pip install emoji\n",
        "!pip install wordsegment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnq4uL8OZ9gz",
        "colab_type": "text"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uus3mquzgUWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import emoji\n",
        "from wordsegment import load, segment\n",
        "load()\n",
        "\n",
        "files_to_preprocess = [\"training\", \"test\"]\n",
        "# preprocessing options\n",
        "demojize_tweet = False\n",
        "desegmentize_hashtags_in_tweet = True\n",
        "\n",
        "\n",
        "def demojize(tweet):\n",
        "    return emoji.demojize(tweet).replace(\":\", \" \").replace(\"_\", \" \")\n",
        "\n",
        "\n",
        "def desegmentize_hashtags(tweet):\n",
        "    new_tweet = []\n",
        "    for word in tweet.split():\n",
        "        new_word = word\n",
        "        if word[0] == '#':\n",
        "            new_word = \" \".join(segment(word[1:]))\n",
        "        new_tweet.append(new_word)\n",
        "    return \" \".join(new_tweet)\n",
        "\n",
        "\n",
        "for file_to_preprocess in files_to_preprocess:\n",
        "    lines = []\n",
        "    preprocessed_lines = []\n",
        "\n",
        "    with open(file_to_preprocess + \".tsv\", \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)    \n",
        "        for line in reader:\n",
        "            lines.append(line)\n",
        "\n",
        "    with open(file_to_preprocess + \"_preprocessed.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in lines:\n",
        "            preprocessed_line = line\n",
        "            \n",
        "            if demojize_tweet:\n",
        "                preprocessed_line[1] = demojize(preprocessed_line[1])\n",
        "            \n",
        "            if desegmentize_hashtags_in_tweet:\n",
        "                preprocessed_line[1] = desegmentize_hashtags(preprocessed_line[1])\n",
        "\n",
        "            preprocessed_lines.append(preprocessed_line)\n",
        "        \n",
        "        \n",
        "        writer = csv.writer(f, delimiter=\"\\t\", quotechar=None)\n",
        "        for line in preprocessed_lines:\n",
        "            writer.writerow(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFlMy-DxAC6J",
        "colab_type": "text"
      },
      "source": [
        "# Model definition and data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-__iJ5A3uDpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import csv\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from pytorch_pretrained_bert.optimization import BertAdam\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
        "\n",
        "\n",
        "class InputExample(object):\n",
        "    def __init__(self, guid, text, label=None):\n",
        "        self.guid = guid\n",
        "        self.text = text\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    def __init__(self, input_ids, attention_mask, label):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "    def get_train_examples(self, training_data):\n",
        "        return self._create_trn_examples(self._read_tsv(training_data))\n",
        "\n",
        "    def get_eval_examples(self, eval_data):\n",
        "        return self._create_eval_examples(self._read_tsv(eval_data))\n",
        "\n",
        "    def get_labels(self):\n",
        "        return [\"0\", \"1\"]\n",
        "\n",
        "    def _create_trn_examples(self, lines):\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if i != 0:\n",
        "                guid = line[0]\n",
        "                text = line[1]\n",
        "                if line[2] == \"OFF\":\n",
        "                    label = \"1\"\n",
        "                else:\n",
        "                    label = \"0\"\n",
        "                examples.append(InputExample(guid=guid, text=text, label=label))\n",
        "        return examples\n",
        "\n",
        "    def _create_eval_examples(self, lines):\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if i != 0:\n",
        "                guid = line[0]\n",
        "                text = line[1]\n",
        "                if line[2] == \"OFF\":\n",
        "                    label = \"1\"\n",
        "                else:\n",
        "                    label = \"0\"\n",
        "                examples.append(InputExample(guid=guid, text=text, label=label))\n",
        "        return examples\n",
        "\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
        "    label_map = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        tokens = tokenizer.tokenize(example.text)\n",
        "\n",
        "        # Account for [CLS] and [SEP] with \"- 2\"\n",
        "        if len(tokens) > max_seq_length - 2:\n",
        "            tokens = tokens[: (max_seq_length - 2)]\n",
        "\n",
        "        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        attention_mask += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(attention_mask) == max_seq_length\n",
        "\n",
        "        label = label_map[example.label]\n",
        "        if ex_index < 0:\n",
        "            print(\"\\n*** Example ***\")\n",
        "            print(\"guid: %s\" % (example.guid))\n",
        "            print(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
        "            print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            print(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
        "            print(\"label: %s (id = %d)\" % (example.label, label))\n",
        "\n",
        "        features.append(InputFeatures(input_ids=input_ids, attention_mask=attention_mask, label=label))\n",
        "    return features\n",
        "\n",
        "\n",
        "def accuracy(out, gold):\n",
        "    guess = np.argmax(out, axis=1)\n",
        "    return (guess == gold).sum()\n",
        "\n",
        "\n",
        "def stats(out, gold):\n",
        "    guess = np.argmax(out, axis=1)\n",
        "\n",
        "    tp = (gold * guess).sum()\n",
        "    tn = ((1 - gold) * (1 - guess)).sum()\n",
        "    fp = ((1 - gold) * guess).sum()\n",
        "    fn = (gold * (1 - guess)).sum()\n",
        "\n",
        "    return tp, tn, fp, fn\n",
        "\n",
        "\n",
        "def warmup_linear(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x / warmup\n",
        "    return 1.0 - x\n",
        "\n",
        "\n",
        "def main(\n",
        "    bert_model=\"bert-base-uncased\",\n",
        "    distil=False,\n",
        "    max_seq_length=80,\n",
        "    do_train=True,\n",
        "    training_data=\"training.tsv\",\n",
        "    do_eval=True,\n",
        "    eval_data=\"test.tsv\",\n",
        "    lower_case=True,\n",
        "    train_batch_size=32,\n",
        "    eval_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=2.0,\n",
        "    warmup_proportion=0.1,\n",
        "    seed=42,\n",
        "    use_extra_tokens=False,\n",
        "):\n",
        "\n",
        "    output_dir = \"./output/\"\n",
        "    extra_tokens = ['MAGA', 'antifa', 'lol', 'Kavanaugh', 'GOP','tweet', 'NRA', 'Dems', 'WWG1WGA', 'nigga', 'WalkAway']\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    print(f\"device: {device}\")\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    processor = DataProcessor()\n",
        "    num_labels = 2\n",
        "    label_list = processor.get_labels()\n",
        "\n",
        "    train_examples = None\n",
        "    num_train_steps = None\n",
        "    if do_train:\n",
        "        train_examples = processor.get_train_examples(training_data)\n",
        "        num_train_steps = int(len(train_examples) / train_batch_size * num_train_epochs)\n",
        "\n",
        "    # Prepare model\n",
        "    if distil:\n",
        "        bert_model = \"distil\" + bert_model\n",
        "        tokenizer = DistilBertTokenizer.from_pretrained(bert_model, do_lower_case=lower_case)\n",
        "        model = DistilBertForSequenceClassification.from_pretrained(bert_model, num_labels=num_labels)\n",
        "    else:\n",
        "        tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=lower_case)\n",
        "        model = BertForSequenceClassification.from_pretrained(bert_model, num_labels=num_labels)\n",
        "\n",
        "    if use_extra_tokens:\n",
        "        tokenizer.add_tokens(extra_tokens)\n",
        "    model.resize_token_embeddings(len(tokenizer)) \n",
        "    model.to(device)\n",
        "\n",
        "    global_step = 0\n",
        "    nb_tr_steps = 0\n",
        "    tr_loss = 0\n",
        "    if do_train:\n",
        "        # Prepare optimizer\n",
        "        param_optimizer = list(model.named_parameters())\n",
        "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "            {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        t_total = num_train_steps\n",
        "        optimizer = BertAdam(optimizer_grouped_parameters, lr=learning_rate, warmup=warmup_proportion, t_total=t_total)\n",
        "\n",
        "        train_features = convert_examples_to_features(train_examples, label_list, max_seq_length, tokenizer)\n",
        "        print(\"\\n\\n***** Running training *****\")\n",
        "        print(f\"  Num examples = {len(train_examples)}\")\n",
        "        print(f\"  Batch size = {train_batch_size}\")\n",
        "        print(f\"  Num steps = {num_train_steps}\")\n",
        "        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "        all_attention_mask = torch.tensor([f.attention_mask for f in train_features], dtype=torch.long)\n",
        "        all_labels = torch.tensor([f.label for f in train_features], dtype=torch.long)\n",
        "        train_data = TensorDataset(all_input_ids, all_attention_mask, all_labels)\n",
        "        train_sampler = RandomSampler(train_data)\n",
        "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(int(num_train_epochs)):\n",
        "            print(f\"\\nepoch {epoch + 1} of {int(num_train_epochs)}:\", flush=True)\n",
        "            tr_loss = 0\n",
        "            nb_tr_examples, nb_tr_steps = 0, 0\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                sys.stdout.write(\"\\r\")\n",
        "                sys.stdout.write(f\"iteration {step:03} of {int(t_total / num_train_epochs):03}\")\n",
        "                sys.stdout.flush()\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                input_ids, attention_mask, labels = batch\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs[0]\n",
        "                loss.backward()\n",
        "\n",
        "                tr_loss += loss.item()\n",
        "                nb_tr_examples += input_ids.size(0)\n",
        "                nb_tr_steps += 1\n",
        "\n",
        "                # modify learning rate with special warm up BERT uses\n",
        "                lr_this_step = learning_rate * warmup_linear(global_step / t_total, warmup_proportion)\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group[\"lr\"] = lr_this_step\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "    # Save a trained model\n",
        "    model_to_save = model.module if hasattr(model, \"module\") else model  # Only save the model it-self\n",
        "    output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
        "    if do_train:\n",
        "        torch.save(model_to_save.state_dict(), output_model_file)\n",
        "\n",
        "    # Load a trained model\n",
        "    model_state_dict = torch.load(output_model_file)\n",
        "    if distil:\n",
        "        model = DistilBertForSequenceClassification.from_pretrained(bert_model, num_labels=num_labels)\n",
        "    else:\n",
        "        model = BertForSequenceClassification.from_pretrained(bert_model, num_labels=num_labels)\n",
        "    model.resize_token_embeddings(len(tokenizer)) \n",
        "    model.load_state_dict(model_state_dict)\n",
        "    model.to(device)\n",
        "\n",
        "    if do_eval:\n",
        "        eval_examples = processor.get_eval_examples(eval_data)\n",
        "        eval_features = convert_examples_to_features(eval_examples, label_list, max_seq_length, tokenizer)\n",
        "        print(\"\\n\\n\\n***** Running evaluation *****\")\n",
        "        print(f\"  Num examples = {len(eval_examples)}\")\n",
        "        print(f\"  Batch size = {eval_batch_size}\\n\")\n",
        "        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
        "        all_attention_mask = torch.tensor([f.attention_mask for f in eval_features], dtype=torch.long)\n",
        "        all_labels = torch.tensor([f.label for f in eval_features], dtype=torch.long)\n",
        "        eval_data = TensorDataset(all_input_ids, all_attention_mask, all_labels)\n",
        "        eval_sampler = SequentialSampler(eval_data)\n",
        "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
        "\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy, = 0, 0\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\n",
        "        nb_total_eval_steps = int(np.ceil(len(eval_examples) / eval_batch_size))\n",
        "        eval_tp, eval_tn, eval_fp, eval_fn = 0, 0, 0, 0\n",
        "\n",
        "        for input_ids, attention_mask, labels in eval_dataloader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            labels = labels.to(\"cpu\").numpy()\n",
        "            tmp_eval_accuracy = accuracy(logits, labels)\n",
        "            tmp_eval_tp, tmp_eval_tn, tmp_eval_fp, tmp_eval_fn = stats(logits, labels)\n",
        "            eval_tp += tmp_eval_tp\n",
        "            eval_tn += tmp_eval_tn\n",
        "            eval_fp += tmp_eval_fp\n",
        "            eval_fn += tmp_eval_fn\n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "            nb_eval_examples += input_ids.size(0)\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "            sys.stdout.write(\"\\r\")\n",
        "            sys.stdout.write(f\"iteration {nb_eval_steps:03} of {nb_total_eval_steps:03}\")\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        eval_loss = eval_loss / nb_eval_steps\n",
        "        eval_accuracy = eval_accuracy / nb_eval_examples\n",
        "        loss = tr_loss / nb_tr_steps if do_train else None\n",
        "\n",
        "        epsilon = 1e-7\n",
        "\n",
        "        offensive_precision = eval_tp / (eval_tp + eval_fp + epsilon)\n",
        "        offensive_recall = eval_tp / (eval_tp + eval_fn + epsilon)\n",
        "\n",
        "        not_offensive_precision = eval_tn / (eval_tn + eval_fn + epsilon)\n",
        "        not_offensive_recall = eval_tn / (eval_tn + eval_fp + epsilon)\n",
        "\n",
        "        offensive_f1 = 2 * (offensive_precision * offensive_recall) / (offensive_precision + offensive_recall + epsilon)\n",
        "        not_offensive_f1 = 2 * (not_offensive_precision * not_offensive_recall) / (not_offensive_precision + not_offensive_recall + epsilon)\n",
        "        f1 = (offensive_f1 + not_offensive_f1) / 2\n",
        "\n",
        "        result = {\"eval_loss\": eval_loss, \"eval_accuracy\": eval_accuracy, \"global_step\": global_step, \"loss\": loss, \"f1_score\": f1}\n",
        "\n",
        "        output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"w\") as writer:\n",
        "            print(\"\\n\\n***** Results *****\")\n",
        "            for key in sorted(result.keys()):\n",
        "                print(f\"  {key} = {result[key]}\")\n",
        "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "        \n",
        "        return f1, eval_tp, eval_tn, eval_fp, eval_fn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91TONFcKANVy",
        "colab_type": "text"
      },
      "source": [
        "# Running the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf-ZMxHsPOuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DTgORE9K1zm",
        "colab_type": "code",
        "outputId": "a44b9068-19e4-4046-97fa-9716b46676de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "runs = 5\n",
        "f1s, tps, tns, fps, fns = 0, 0, 0, 0, 0\n",
        "\n",
        "for i in range(runs):\n",
        "    f1, tp, tn, fp, fn = main(distil=True, seed=666 * i, training_data=\"training_preprocessed.tsv\", eval_data=\"test_preprocessed.tsv\", use_extra_tokens=True)\n",
        "    f1s += f1\n",
        "    tps += tp\n",
        "    tns += tn\n",
        "    fps += fp\n",
        "    fns += fn\n",
        "\n",
        "print(f\"average f1-score: {f1s/runs}\")\n",
        "print(f\"average true pos: {tps/runs}\")\n",
        "print(f\"average true neg: {tns/runs}\")\n",
        "print(f\"average false pos: {fps/runs}\")\n",
        "print(f\"average false neg: {fns/runs}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cuda\n",
            "\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 13240\n",
            "  Batch size = 32\n",
            "  Num steps = 827\n",
            "\n",
            "epoch 1 of 2:\n",
            "iteration 413 of 413\n",
            "epoch 2 of 2:\n",
            "iteration 413 of 413\n",
            "\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = 860\n",
            "  Batch size = 8\n",
            "\n",
            "iteration 108 of 108\n",
            "\n",
            "***** Results *****\n",
            "  eval_accuracy = 0.8593023255813953\n",
            "  eval_loss = 0.3575202508105172\n",
            "  f1_score = 0.8192068299288635\n",
            "  global_step = 828\n",
            "  loss = 0.3561617328924833\n",
            "device: cuda\n",
            "\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 13240\n",
            "  Batch size = 32\n",
            "  Num steps = 827\n",
            "\n",
            "epoch 1 of 2:\n",
            "iteration 413 of 413\n",
            "epoch 2 of 2:\n",
            "iteration 413 of 413\n",
            "\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = 860\n",
            "  Batch size = 8\n",
            "\n",
            "iteration 108 of 108\n",
            "\n",
            "***** Results *****\n",
            "  eval_accuracy = 0.8569767441860465\n",
            "  eval_loss = 0.35229637301354494\n",
            "  f1_score = 0.8197092143575966\n",
            "  global_step = 828\n",
            "  loss = 0.3523307452203283\n",
            "device: cuda\n",
            "\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 13240\n",
            "  Batch size = 32\n",
            "  Num steps = 827\n",
            "\n",
            "epoch 1 of 2:\n",
            "iteration 413 of 413\n",
            "epoch 2 of 2:\n",
            "iteration 413 of 413\n",
            "\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = 860\n",
            "  Batch size = 8\n",
            "\n",
            "iteration 108 of 108\n",
            "\n",
            "***** Results *****\n",
            "  eval_accuracy = 0.8534883720930233\n",
            "  eval_loss = 0.3531421162877922\n",
            "  f1_score = 0.815065076357479\n",
            "  global_step = 828\n",
            "  loss = 0.3561949010416505\n",
            "device: cuda\n",
            "\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 13240\n",
            "  Batch size = 32\n",
            "  Num steps = 827\n",
            "\n",
            "epoch 1 of 2:\n",
            "iteration 413 of 413\n",
            "epoch 2 of 2:\n",
            "iteration 413 of 413\n",
            "\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = 860\n",
            "  Batch size = 8\n",
            "\n",
            "iteration 108 of 108\n",
            "\n",
            "***** Results *****\n",
            "  eval_accuracy = 0.8534883720930233\n",
            "  eval_loss = 0.3579598902552216\n",
            "  f1_score = 0.8130434280851568\n",
            "  global_step = 828\n",
            "  loss = 0.35350818824076996\n",
            "device: cuda\n",
            "\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 13240\n",
            "  Batch size = 32\n",
            "  Num steps = 827\n",
            "\n",
            "epoch 1 of 2:\n",
            "iteration 413 of 413\n",
            "epoch 2 of 2:\n",
            "iteration 413 of 413\n",
            "\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = 860\n",
            "  Batch size = 8\n",
            "\n",
            "iteration 108 of 108\n",
            "\n",
            "***** Results *****\n",
            "  eval_accuracy = 0.8534883720930233\n",
            "  eval_loss = 0.3536467755696288\n",
            "  f1_score = 0.8155573965937686\n",
            "  global_step = 828\n",
            "  loss = 0.3613221361009395\n",
            "average f1-score: 0.8165163890645729\n",
            "average tp: 170.0\n",
            "average tn: 565.6\n",
            "average fp: 54.4\n",
            "average fn: 70.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}