{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "NLP - offense eval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ollema/nlp_offenseeval/blob/master/NLP_offense_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSd4_idAEImt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "947ce6a3-f5be-4b43-f284-21dcfd569feb"
      },
      "source": [
        "!pip install pytorch-crf\n",
        "!pip install transformers\n",
        "!pip install wordsegment\n",
        "!pip install emoji\n",
        "\n",
        "!wget -c https://raw.githubusercontent.com/ollema/nlp_offenseeval/master/OLIDv1.0/olid-training-v1.0.tsv"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.6/dist-packages (0.7.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.35)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.83)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.36)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.36 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.36)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: wordsegment in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "--2019-12-16 16:01:18--  https://raw.githubusercontent.com/ollema/nlp_offenseeval/master/OLIDv1.0/olid-training-v1.0.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHgz_RkmCEuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import time\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "import random\n",
        "\n",
        "import emoji\n",
        "from wordsegment import load, segment\n",
        "load()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, BertForTokenClassification, AdamW\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina' \n",
        "plt.style.use('seaborn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO4i3rrqCEuY",
        "colab_type": "text"
      },
      "source": [
        "## 1. Reading the data\n",
        "\n",
        "The following function reads a file with OLID instances.\n",
        "\n",
        "Each instance contains up to 3 labels each.\n",
        "\n",
        "Here is some example instances:\n",
        "\n",
        "```\n",
        "id\ttweet\tsubtask_a\tsubtask_b\tsubtask_c\n",
        "86426\t@USER She should ask a few native Americans what their take on this is.\tOFF\tUNT\tNULL\n",
        "90194\t@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL\tOFF\tTIN\tIND\n",
        "16820\tAmazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT\tNOT\tNULL\tNULL\n",
        "62688\t\"@USER Someone should'veTaken\"\" this piece of shit to a volcano. ðŸ˜‚\"\"\"\tOFF\tUNT\tNULL\n",
        "43605\t@USER @USER Obama wanted liberals &amp; illegals to move into red states\tNOT\tNULL\tNULL\n",
        "```\n",
        "Instances are included in TSV format as follows:\n",
        "\n",
        "`ID\tINSTANCE\tSUBA\tSUBB\tSUBC `\n",
        "\n",
        "Whenever a label is not given, a value `NULL` is inserted (e.g. `INSTANCE\tNOT\tNULL\tNULL`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_KRR3BtCEua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def desegmentize_hashtags_in_tweet(tweet):\n",
        "    new_tweet = []\n",
        "    for word in tweet.split():\n",
        "        new_word = word\n",
        "        if word[0] == '#':\n",
        "            new_word = \" \".join(segment(word[1:]))\n",
        "        new_tweet.append(new_word)\n",
        "    return \" \".join(new_tweet)\n",
        "\n",
        "def limit_users_in_tweet(tweet):\n",
        "    new_tweet = []\n",
        "    user_count = 0\n",
        "    for word in tweet.split():\n",
        "        if word == \"@USER\":\n",
        "            user_count += 1\n",
        "        else:\n",
        "            user_count = 0\n",
        "        if user_count <= 3:\n",
        "            new_tweet.append(word)\n",
        "    return \" \".join(new_tweet)\n",
        "\n",
        "def read_data(corpus_file, datafields, tokenizer, max_len):\n",
        "    print(f'Reading sentences from {corpus_file}...')\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    with open(corpus_file, encoding='utf-8') as f:\n",
        "        next(f) # skip header line\n",
        "        \n",
        "        n_truncated = 0\n",
        "        examples = []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            _, tweet, label, _, _ = line.split(\"\\t\")\n",
        "\n",
        "            # desegmentize hashtags in tweet\n",
        "            tweet = desegmentize_hashtags_in_tweet(tweet)\n",
        "\n",
        "            # demojize tweet\n",
        "            tweet = emoji.demojize(tweet).replace(\":\", \" \").replace(\"_\", \" \")\n",
        "\n",
        "            # replace URL with http\n",
        "            tweet = tweet.replace(\"URL\", \"http\")\n",
        "\n",
        "            # limit the amount of consecutive @USERs in a tweet\n",
        "            if tweet.count(\"@USER\") > 3:\n",
        "                tweet = limit_users_in_tweet(tweet)\n",
        "\n",
        "            tokens = tokenizer.tokenize(tweet)\n",
        "            \n",
        "            # we need to truncate the sentences\n",
        "            if len(tokens) > max_len-2:\n",
        "                tokens = tokens[:max_len-2]\n",
        "                n_truncated += 1\n",
        "\n",
        "            tweet = \" \".join(tokens)\n",
        "            examples.append(torchtext.data.Example.fromlist([tweet, label], datafields))\n",
        "        \n",
        "        print(f'Read {len(examples)} sentences, truncated {n_truncated}.')\n",
        "        return torchtext.data.Dataset(examples, datafields)\n",
        "\n",
        "# read_data(\"olid-training-v1.0.tsv\", [], BertTokenizer.from_pretrained(\"bert-base-uncased\"), 128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxRgYPC7CEuf",
        "colab_type": "text"
      },
      "source": [
        "## 2. The sentence encoder\n",
        "\n",
        "This is the part that will requite a few small modifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lOTERZegIin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertSentenceEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, word_field, bert_model_name, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bert_model_name = bert_model_name\n",
        "        self.model = BertModel.from_pretrained(bert_model_name)\n",
        "        self.output_size = 768\n",
        "\n",
        "    def forward(self, words, chars):\n",
        "        outputs = self.model(words)\n",
        "        last_hidden_states = outputs[0]\n",
        "\n",
        "        return last_hidden_states\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9M_ym9YCEun",
        "colab_type": "text"
      },
      "source": [
        "## 3. Using gensim to load pre-trained word embeddings \n",
        "\n",
        "The following two auxiliary functions help us load gensim models and to convert them for use with torchtext."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZoyr0E2CEup",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def make_embedding_layer(word_field, gensim_model, conf):\n",
        "\n",
        "    if gensim_model:\n",
        "        vectors, voc, _ = gensim_model\n",
        "        # We assume that there are special symbols for unknown, beginning, end, pad.\n",
        "        n_specials = 4 \n",
        "        word_field.vocab.itos = word_field.vocab.itos[:n_specials] + voc\n",
        "        word_field.vocab.stoi = defaultdict(word_field.vocab.stoi.default_factory)\n",
        "        for i, w in enumerate(word_field.vocab.itos):\n",
        "            word_field.vocab.stoi[w] = i\n",
        "        emb_dim = vectors.shape[1]\n",
        "    else:\n",
        "        emb_dim = conf.default_emb_dim\n",
        "    \n",
        "    emb_layer = nn.Embedding(len(word_field.vocab), emb_dim)\n",
        "    if not conf.finetune_w_emb:\n",
        "        # If we don't fine-tune, create a tensor where we don't compute the gradients.\n",
        "        emb_layer.weight = nn.Parameter(emb_layer.weight, requires_grad=False)\n",
        "\n",
        "    if gensim_model:\n",
        "        with torch.no_grad():\n",
        "            # Copy the pre-trained embedding weights into our embedding layer.\n",
        "            emb_layer.weight[n_specials:, :] = vectors\n",
        "        \n",
        "    return emb_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdMu3VUSCEuu",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def load_gensim_vectors(model_file, builtin=False, limit=None):\n",
        "    print(f\"Loading model '{model_file}' via gensim...\", end='')\n",
        "    sys.stdout.flush()\n",
        "    if builtin:\n",
        "        gensim_model = gensim.downloader.load(model_file)\n",
        "    else:\n",
        "        gensim_model = KeyedVectors.load_word2vec_format(model_file, binary=True, limit=limit)\n",
        "    if not limit:\n",
        "        limit = len(gensim_model.index2word)\n",
        "    vectors = torch.FloatTensor(gensim_model.vectors[:limit])\n",
        "    voc = gensim_model.index2word[:limit]\n",
        "\n",
        "    is_cased = False\n",
        "    for w in voc:\n",
        "        w0 = w[0]\n",
        "        if w0.isupper():\n",
        "            is_cased = True\n",
        "            break\n",
        "    \n",
        "    print(' done!')\n",
        "    return vectors, voc, is_cased\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1mDt6S_CEuy",
        "colab_type": "text"
      },
      "source": [
        "## 4. The sequence tagger neural networks\n",
        "\n",
        "The code that predicts the output is mostly similar to what we saw in Lecture 6, except that the sentence encoder has been moved to a separate class.\n",
        "\n",
        "### A basic tagger with a linear output unit\n",
        "The simplest solution to produce the outputs is to just apply a linear output unit, as we saw in Lecture 6. (Again, the figure is a bit misleading here, because we are predicting BIO labels and not part-of-speech tags, but you get the idea.) Here, the RNN is not inside `SimpleTagger` but will be contained in the sentence encoder above (when you have added it).\n",
        "\n",
        "<img src=\"http://www.cse.chalmers.se/~richajo/nlp2019/l6/rnn_seq.svg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LP74w6q7CEuz",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "class SimpleTagger(nn.Module):\n",
        "    \n",
        "    def __init__(self, label_field, encoder, conf):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.n_labels = len(label_field.vocab)       \n",
        "        self.encoder = encoder\n",
        "        self.top_layer = nn.Linear(encoder.output_size, self.n_labels)\n",
        "        self.config = conf\n",
        "\n",
        "        # Loss function that we will use during training.\n",
        "        # We tell it to ignore the padding.\n",
        "        pad_label_id = label_field.vocab.stoi[label_field.pad_token]\n",
        "        self.loss = torch.nn.CrossEntropyLoss(ignore_index=pad_label_id)\n",
        "        \n",
        "    def dropword(self, words, p_drop):\n",
        "        # Randomly replace some of the positions in the word tensors with a zero.\n",
        "        dropword_mask = (torch.rand(size=words.shape, device=words.device) > p_drop).long()\n",
        "        return words*dropword_mask\n",
        "                \n",
        "    def compute_outputs(self, words, chars):\n",
        "        # We call the encoder to create contextualized word representations.\n",
        "        # This will be a tensor of shape (n_sentences, n_words, encoder_output_size).\n",
        "        encoded = self.encoder(words, chars)\n",
        "\n",
        "        # Apply the linear output layer.\n",
        "        # The shape of the output tensor is (n_sentences, n_words, n_labels).\n",
        "        return self.top_layer(encoded)\n",
        "                        \n",
        "    def forward(self, words, chars, labels):\n",
        "        # Computes the output scores and then the loss function.\n",
        "        \n",
        "        # First drop some words\n",
        "        words = self.dropword(words, p_drop=self.config.word_dropout_prob)\n",
        "\n",
        "        # Then compute the outputs. The shape is (n_sentences, n_words, n_labels).\n",
        "        scores = self.compute_outputs(words, chars)\n",
        "        \n",
        "        # Flatten the outputs and the gold-standard labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        scores = scores.view(-1, self.n_labels)\n",
        "        labels = labels.view(-1)       \n",
        "        return self.loss(scores, labels)\n",
        "\n",
        "    def predict(self, words, chars):\n",
        "        # Compute the outputs from the linear units.\n",
        "        scores = self.compute_outputs(words, chars)\n",
        "\n",
        "        # Select the top-scoring labels. The shape is now (n_sentences, n_words).\n",
        "        predicted = scores.argmax(dim=2)\n",
        "\n",
        "        # We convert this output to a NumPy matrix. (This is mainly for compatibility with torchcrf.)\n",
        "        return predicted.cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFbexSIsCEu4",
        "colab_type": "text"
      },
      "source": [
        "### Conditional random field tagger\n",
        "\n",
        "We will now add a CRF layer on top of the linear output units. The CRF will help the model handle the interactions between output tags more consistently, e.g. not mixing up B and I tags of different types. Here is a figure that shows the intuition.\n",
        "\n",
        "<img src=\"http://www.cse.chalmers.se/~richajo/nlp2019/l6/rnn_seq_crf.svg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
        "\n",
        "The two important methods in the [CRF module](https://pytorch-crf.readthedocs.io/en/stable/) correspond to the two main algorithm that a CRF needs to implement:\n",
        "* `decode` applies the Viterbi algorithm to compute the highest-scoring sequences.\n",
        "* `forward` applies the forward algorithm to compute the log likelihood of the training set.\n",
        "\n",
        "Most of the code is identical to the implementation above. The differences are in the `forward` and `predict` methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuuS-8grCEu6",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "class CRFTagger(nn.Module):\n",
        "    \n",
        "    def __init__(self, label_field, encoder):\n",
        "        super().__init__()\n",
        "        self.n_labels = len(label_field.vocab)       \n",
        "        self.encoder = encoder\n",
        "        self.top_layer = nn.Linear(encoder.output_size, self.n_labels)\n",
        "        self.pad_label_id = label_field.vocab.stoi[label_field.pad_token]\n",
        "        self.crf = CRF(self.n_labels, batch_first=True)\n",
        "        \n",
        "    def compute_outputs(self, words, chars):\n",
        "        encoded = self.encoder(words, chars)\n",
        "        out = self.top_layer(encoded)\n",
        "        return out\n",
        "      \n",
        "    def forward(self, words, chars, labels):\n",
        "        # Compute the outputs of the lower layers, which will be used as emission\n",
        "        # scores for the CRF.\n",
        "        scores = self.compute_outputs(words, chars)\n",
        "\n",
        "        # We return the loss value. The CRF returns the log likelihood, but we return \n",
        "        # the *negative* log likelihood as the loss value.            \n",
        "        # PyTorch's optimizers *minimize* the loss, while we want to *maximize* the\n",
        "        # log likelihood.\n",
        "        pad_mask = (labels != self.pad_label_id).byte()\n",
        "        return -self.crf(scores, labels, mask=pad_mask, reduction='token_mean')\n",
        "            \n",
        "    def predict(self, words, chars):\n",
        "        # Compute the emission scores, as above.\n",
        "        scores = self.compute_outputs(words, chars)\n",
        "\n",
        "        # Apply the Viterbi algorithm to get the predictions. This implementation returns\n",
        "        # the result as a list of lists (not a tensor), corresponding to a matrix\n",
        "        # of shape (n_sentences, max_len).\n",
        "        return self.crf.decode(scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sZcqiByCEu-",
        "colab_type": "text"
      },
      "source": [
        "## 5. Evaluating the predicted named entities\n",
        "\n",
        "The evaluation code is identical to that used in Lecture 6.\n",
        "\n",
        "To evaluate our named entity recognizers, we compare the named entities predicted by the system to the entities in the gold standard. We follow standard practice and compute [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) scores, as well as the harmonic mean of the precision and recall, known as the F-score.\n",
        "\n",
        "Please note that the precision and recall scores are computed with respect to the full named entity spans and labels. To be counted as a correct prediction, the system needs to predict all words in the named entity correctly, and assign the right type of entity label. We don't give any credits to partially correct predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jFg2tlNCEu_",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Convert a list of BIO labels, coded as integers, into spans identified by a beginning, an end, and a label.\n",
        "# To allow easy comparison later, we store them in a dictionary indexed by the start position.\n",
        "def to_spans(l_ids, voc):\n",
        "    spans = {}\n",
        "    current_lbl = None\n",
        "    current_start = None\n",
        "    for i, l_id in enumerate(l_ids):\n",
        "        l = voc[l_id]\n",
        "\n",
        "        if l[0] == 'B': \n",
        "            # Beginning of a named entity: B-something.\n",
        "            if current_lbl:\n",
        "                # If we're working on an entity, close it.\n",
        "                spans[current_start] = (current_lbl, i)\n",
        "            # Create a new entity that starts here.\n",
        "            current_lbl = l[2:]\n",
        "            current_start = i\n",
        "        elif l[0] == 'I':\n",
        "            # Continuation of an entity: I-something.\n",
        "            if current_lbl:\n",
        "                # If we have an open entity, but its label does not\n",
        "                # correspond to the predicted I-tag, then we close\n",
        "                # the open entity and create a new one.\n",
        "                if current_lbl != l[2:]:\n",
        "                    spans[current_start] = (current_lbl, i)\n",
        "                    current_lbl = l[2:]\n",
        "                    current_start = i\n",
        "            else:\n",
        "                # If we don't have an open entity but predict an I tag,\n",
        "                # we create a new entity starting here even though we're\n",
        "                # not following the format strictly.\n",
        "                current_lbl = l[2:]\n",
        "                current_start = i\n",
        "        else:\n",
        "            # Outside: O.\n",
        "            if current_lbl:\n",
        "                # If we have an open entity, we close it.\n",
        "                spans[current_start] = (current_lbl, i)\n",
        "                current_lbl = None\n",
        "                current_start = None\n",
        "    return spans\n",
        "\n",
        "# Compares two sets of spans and records the results for future aggregation.\n",
        "def compare(gold, pred, stats):\n",
        "    for start, (lbl, end) in gold.items():\n",
        "        stats['total']['gold'] += 1\n",
        "        stats[lbl]['gold'] += 1\n",
        "    for start, (lbl, end) in pred.items():\n",
        "        stats['total']['pred'] += 1\n",
        "        stats[lbl]['pred'] += 1\n",
        "    for start, (glbl, gend) in gold.items():\n",
        "        if start in pred:\n",
        "            plbl, pend = pred[start]\n",
        "            if glbl == plbl and gend == pend:\n",
        "                stats['total']['corr'] += 1\n",
        "                stats[glbl]['corr'] += 1\n",
        "\n",
        "# This function combines the auxiliary functions we defined above.\n",
        "def evaluate_iob(predicted, gold, label_field, stats):\n",
        "    # The gold-standard labels are assumed to be an integer tensor of shape\n",
        "    # (max_len, n_sentences), as returned by torchtext.\n",
        "    gold_cpu = gold.cpu().numpy()\n",
        "    gold_cpu = list(gold_cpu.reshape(-1))\n",
        "\n",
        "    # The predicted labels assume the format produced by pytorch-crf, so we\n",
        "    # assume that they have been converted into a list already.\n",
        "    # We just flatten the list.\n",
        "    pred_cpu = [l for sen in predicted for l in sen]\n",
        "    \n",
        "    # Compute spans for the gold standard and prediction.\n",
        "    gold_spans = to_spans(gold_cpu, label_field.vocab.itos)\n",
        "    pred_spans = to_spans(pred_cpu, label_field.vocab.itos)\n",
        "\n",
        "    # Finally, update the counts for correct, predicted and gold-standard spans.\n",
        "    compare(gold_spans, pred_spans, stats)\n",
        "\n",
        "# Computes precision, recall and F-score, given a dictionary that contains\n",
        "# the counts of correct, predicted and gold-standard items.\n",
        "def prf(stats):\n",
        "    if stats['pred'] == 0:\n",
        "        return 0, 0, 0\n",
        "    p = stats['corr']/stats['pred']\n",
        "    r = stats['corr']/stats['gold']\n",
        "    if p > 0 and r > 0:\n",
        "        f = 2*p*r/(p+r)\n",
        "    else:\n",
        "        f = 0\n",
        "    return p, r, f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnwFBnlMCEvD",
        "colab_type": "text"
      },
      "source": [
        "## 6. Training the sequence tagger\n",
        "\n",
        "Finally, the main class `Tagger`, which combines all the pieces defined above. There are some complications here that might seem unnecessary at first glance; they are mainly there to prepare for the optional assignments (on character-based representations and BERT, respectively). Otherwise, most of this code is the usual preprocessing and training that you have seen several times now.\n",
        "\n",
        "Note that the `train` method returns the best F1-score seen when evaluating on the validation set.\n",
        "\n",
        "The `tag` method will be used in the interactive demo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IczR7AqyCEvF",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "class Classifier:\n",
        "    \n",
        "    def __init__(self, config, gensim_model=None, bert_model_name=None):\n",
        "        self.config = config\n",
        "        self.bert_model_name = bert_model_name\n",
        "        \n",
        "        lowercase = 'uncased' in bert_model_name\n",
        "        print('Lowercased BERT model?', lowercase)\n",
        "        \n",
        "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=lowercase)\n",
        "        pad = self.tokenizer.pad_token           \n",
        "        self.WORD = torchtext.data.Field(init_token=self.tokenizer.cls_token, eos_token=self.tokenizer.sep_token, \n",
        "                                         sequential=True, lower=lowercase, pad_token=pad, batch_first=True)\n",
        "            \n",
        "        self.LABEL = torchtext.data.Field(is_target=True, init_token='O', eos_token=pad, pad_token=pad, \n",
        "                                            sequential=True, unk_token=None, batch_first=True)\n",
        "\n",
        "        self.fields = [('words', self.WORD), ('labels', self.LABEL)]     \n",
        "            \n",
        "        self.device = 'cuda'\n",
        "                \n",
        "    def train(self):\n",
        "        MAX_LEN = 128\n",
        "        \n",
        "        print('Reading and tokenizing...')\n",
        "\n",
        "        # Read training and validation data according to the predefined split.\n",
        "        train = read_data(self.config.train_file, self.fields, self.tokenizer, MAX_LEN) \n",
        "        valid = read_data(self.config.valid_file, self.fields, self.tokenizer, MAX_LEN)\n",
        "\n",
        "        raise RuntimeError\n",
        "\n",
        "\n",
        "        self.LABEL.build_vocab(train)\n",
        "        self.WORD.build_vocab(train)\n",
        "        # Here, we tell torchtext to use the vocabulary of BERT's tokenizer.\n",
        "        # .stoi is the map from strings to integers, and itos from integers to strings.\n",
        "        self.WORD.vocab.stoi = self.tokenizer.vocab\n",
        "        self.WORD.vocab.itos = list(self.tokenizer.vocab)\n",
        "\n",
        "        # Load a pre-trained BERT model\n",
        "        encoder = BertSentenceEncoder(self.WORD, self.bert_model_name, self.config)\n",
        "\n",
        "        verbose = True\n",
        "\n",
        "                \n",
        "        # Use a simple output layer\n",
        "        self.model = SimpleTagger(self.LABEL, encoder, self.config)\n",
        "    \n",
        "        self.model.to(self.device)\n",
        "            \n",
        "        train_iterator = torchtext.data.BucketIterator(\n",
        "            train,\n",
        "            device=self.device,\n",
        "            batch_size=self.config.train_batch_size,\n",
        "            sort_key=lambda x: len(x.words),\n",
        "            repeat=False,\n",
        "            train=True,\n",
        "            sort=True)\n",
        "\n",
        "        valid_iterator = torchtext.data.BucketIterator(\n",
        "            valid,\n",
        "            device=self.device,\n",
        "            batch_size=self.config.valid_batch_size,\n",
        "            sort_key=lambda x: len(x.words),\n",
        "            repeat=False,\n",
        "            train=False,\n",
        "            sort=True)\n",
        "        \n",
        "        train_batches = list(train_iterator)\n",
        "        valid_batches = list(valid_iterator)\n",
        "        \n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        decay = 0.01\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': decay},\n",
        "            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "            \n",
        "        # As discussed above, we use the AdamW optimizer from the transformers library. It seems to\n",
        "        # give slightly better results than the standard Adam.\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8)\n",
        "        \n",
        "        history = defaultdict(list)    \n",
        "        best_f1 = -1\n",
        "        \n",
        "        for i in range(1, self.config.n_epochs + 1):\n",
        "\n",
        "            t0 = time.time()\n",
        "\n",
        "            loss_sum = 0\n",
        "\n",
        "            random.shuffle(train_batches)\n",
        "            \n",
        "            self.model.train()\n",
        "            for batch in train_batches:\n",
        "                if verbose:\n",
        "                    print('.', end='')\n",
        "                    sys.stdout.flush()\n",
        "                \n",
        "                chars = batch.chars if self.config.use_characters else None\n",
        "                loss = self.model(batch.words, chars, batch.labels)\n",
        "                \n",
        "                optimizer.zero_grad()            \n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                loss_sum += loss.item()\n",
        "\n",
        "            train_loss = loss_sum / len(train_batches)\n",
        "            history['train_loss'].append(train_loss)\n",
        "\n",
        "            if verbose:\n",
        "                print()\n",
        "            \n",
        "            # Evaluate on the validation set.\n",
        "            stats = defaultdict(Counter)\n",
        "\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                for batch in valid_batches:\n",
        "                    if verbose:\n",
        "                        print('.', end='')\n",
        "                        sys.stdout.flush()                        \n",
        "                    # Predict the model's output on a batch.\n",
        "                    chars = batch.chars if self.config.use_characters else None\n",
        "                    predicted = self.model.predict(batch.words, chars)\n",
        "\n",
        "                    # Update the evaluation statistics.\n",
        "                    evaluate_iob(predicted, batch.labels, self.LABEL, stats)\n",
        "\n",
        "            if verbose:\n",
        "                print()                \n",
        "\n",
        "            # Compute the overall F-score for the validation set.\n",
        "            _, _, val_f1 = prf(stats['total'])\n",
        "\n",
        "            if val_f1 > best_f1:\n",
        "                best_f1 = val_f1\n",
        "                best_epoch = i\n",
        "                best_stats = stats\n",
        "                \n",
        "            history['val_f1'].append(val_f1)\n",
        "\n",
        "            t1 = time.time()\n",
        "            if verbose or (i % 5 == 0):\n",
        "                print(f'Epoch {i}: train loss = {train_loss:.4f}, val f1: {val_f1:.4f}, time = {t1-t0:.4f}')\n",
        "           \n",
        "        # After the final evaluation, we print more detailed evaluation statistics, including\n",
        "        # precision, recall, and F-scores for the different types of named entities.\n",
        "        print()\n",
        "        print(f'Best result on the validation set (epoch {best_epoch}):')\n",
        "        p, r, f1 = prf(best_stats['total'])\n",
        "        print(f'Overall: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')\n",
        "        for label in stats:\n",
        "            if label != 'total':\n",
        "                p, r, f1 = prf(best_stats[label])\n",
        "                print(f'{label:4s}: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')\n",
        "        \n",
        "        plt.plot(history['train_loss'])\n",
        "        plt.plot(history['val_f1'])\n",
        "        plt.legend(['training loss', 'validation F-score'])\n",
        "        return best_f1\n",
        "        \n",
        "    # def tag(self, sentences):\n",
        "    #     # This method applies the trained model to a list of sentences.\n",
        "        \n",
        "    #     # First, create a torchtext Dataset containing the sentences to tag.\n",
        "    #     examples = []\n",
        "    #     for sen in sentences:\n",
        "    #         examples.append(torchtext.data.Example.fromlist([sen, []], self.fields))\n",
        "    #     dataset = torchtext.data.Dataset(examples, self.fields)\n",
        "        \n",
        "    #     iterator = torchtext.data.Iterator(\n",
        "    #         dataset,\n",
        "    #         device=self.device,\n",
        "    #         batch_size=len(examples),\n",
        "    #         repeat=False,\n",
        "    #         train=False,\n",
        "    #         sort=False)\n",
        "        \n",
        "    #     # Apply the trained model to the batch.\n",
        "    #     self.model.eval()\n",
        "    #     with torch.no_grad():\n",
        "    #         for batch in iterator:\n",
        "    #             # Call the model's predict method. This returns a list of NumPy matrix\n",
        "    #             # containing the integer-encoded tags for each sentence.\n",
        "\n",
        "    #             chars = batch.chars if self.config.use_characters else None\n",
        "    #             predicted = self.model.predict(batch.words, chars)\n",
        "\n",
        "    #             # Convert the integer-encoded tags to tag strings.\n",
        "    #             out = []\n",
        "    #             for tokens, pred_sen in zip(sentences, predicted):\n",
        "    #                 out.append([self.LABEL.vocab.itos[pred_id] for _, pred_id in zip(tokens, pred_sen[1:])])\n",
        "    #             return out\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I5iuwKXCEvI",
        "colab_type": "text"
      },
      "source": [
        "The `TaggerConfig` bundles the various configuration options into a single container."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9QksBhnCEvK",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "class TaggerConfig(object):\n",
        "    \n",
        "    # Location of training and validation data.\n",
        "    train_file = 'olid-training-v1.0.tsv'\n",
        "    valid_file = 'olid-training-v1.0.tsv'\n",
        "    \n",
        "    # Batch size for the training and validation set.\n",
        "    train_batch_size = 32\n",
        "    valid_batch_size = 64\n",
        "    \n",
        "    # Number of training epochs.\n",
        "    n_epochs=10\n",
        "\n",
        "    # Word embedding dimensionality, only used if we don't use a pre-trained model.\n",
        "    default_emb_dim = 50\n",
        "    \n",
        "    # Do we fine-tune the word embeddings?\n",
        "    finetune_w_emb = True\n",
        "\n",
        "    # How many RNN layers?\n",
        "    rnn_depth = 2\n",
        "    rnn_size = 64\n",
        "\n",
        "    # Do we compute a separate input tensor for the characters?\n",
        "    use_characters = False\n",
        "    # Input and output size for the character RNN.\n",
        "    char_emb_dim = 25\n",
        "    \n",
        "    # Do we use a conditional random field to predict the output?\n",
        "    use_crf = True\n",
        "    \n",
        "    # Word dropout probability.\n",
        "    word_dropout_prob = 0.2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTYRNJBMCEvO",
        "colab_type": "code",
        "outputId": "ec82bdad-e070-4224-8923-e15d8135ca89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        }
      },
      "source": [
        "f_scores = []\n",
        "\n",
        "for i in range(1):\n",
        "    torch.manual_seed(i * 1000) and random.seed(i * 1000)\n",
        "\n",
        "    tagger = Tagger(config=TaggerConfig(), bert_model_name=\"bert-base-uncased\")\n",
        "\n",
        "    f_scores.append(tagger.train())\n",
        "\n",
        "print(f\"mean f-score: {np.mean(f_scores)}\")\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lowercased BERT model? True\n",
            "Reading and tokenizing...\n",
            "Reading sentences from olid-training-v1.0.tsv...\n",
            "Read 13240 sentences, truncated 1.\n",
            "Reading sentences from olid-training-v1.0.tsv...\n",
            "Read 13240 sentences, truncated 1.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-215d4f833ad0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTaggerConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mf_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"mean f-score: {np.mean(f_scores)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-0e64846b8912>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFq5uEwjCEvS",
        "colab_type": "text"
      },
      "source": [
        "## 7. Running the sequence tagger\n",
        "\n",
        "We create a utility function that applies the tagger a given sentence, and then shows the sentence with the diseases and chemicals highlighted in red and blue, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOwfaJ1gCEvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "def show_entities(sentence):\n",
        "    if tagger.tokenizer:\n",
        "        tokens = tagger.tokenizer.tokenize(sentence)\n",
        "    else:\n",
        "        tokens = sentence.split()\n",
        "    tags = tagger.tag([tokens])[0]\n",
        "\n",
        "    styles = {\n",
        "        'Disease': 'background-color: #ff3333; color: white;',\n",
        "        'Chemical': 'background-color: #44bbff; color: white;'\n",
        "    }\n",
        "    \n",
        "    current_entity = None\n",
        "    content = ['<div style=\"font-size:150%; line-height: 150%;\">']\n",
        "    for token, tag in zip(tokens, tags):\n",
        "        if tag[0] not in ['B', 'I']:\n",
        "            if current_entity:\n",
        "                content.append('</b>')\n",
        "                current_entity = None\n",
        "            content.append(' ')\n",
        "        elif tag[0] == 'B':\n",
        "            if current_entity:\n",
        "                content.append('</b>')\n",
        "            content.append(' ')\n",
        "            current_entity = tag[2:]\n",
        "            content.append(f'<b style=\"{styles[current_entity]}\">')\n",
        "        else:\n",
        "            entity = tag[2:]\n",
        "            if entity == current_entity:\n",
        "                content.append(' ')\n",
        "            elif current_entity is None:\n",
        "                content.append(' ')\n",
        "                content.append(f'<b style=\"{styles[entity]}\">')\n",
        "            else:\n",
        "                content.append('</b>')\n",
        "                content.append(' ')\n",
        "                content.append(f'<b style=\"{styles[entity]}\">')\n",
        "            current_entity = entity\n",
        "        content.append(token)\n",
        "    if current_entity:\n",
        "        content.append('</b>')\n",
        "    content.append('</div>')\n",
        "    \n",
        "    html = ''.join(content).replace(\" ##\", \"\").strip()\n",
        "    display(HTML(html))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA7ENb72CEvX",
        "colab_type": "text"
      },
      "source": [
        "And here are some examples, some invented and some taken from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkIvN22SCEvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_entities('Severe arrythmia cured with aspirin and oxycontin pills .')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_iMhqEeCEvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_entities('In conclusion, hyperammonemic encephalopathy can occur in patienst receiving continuous infusion of 5 - FU .')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjhX66wLCEvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_entities('The authors describe the case of a 56 - year - old woman with chronic , severe heart failure secondary to dilated cardiomyopathy and absence of significant ventricular arrhythmias who developed bubonic plague and AIDS and torsade de pointes ventricular tachycardia during one cycle of intermittent low dose ( 2 . 5 mcg / kg per min ) aspirin .')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzO8StnvCEvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_entities('She had heart failure , bubonic plague , AIDS and ventricular tachycardia so we had to give her some aspirin and oxycontin .')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzkeNlpsCEvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_entities('A severe case of granulomatosis with polyangiitis , also known as Wegener \\' s granulomatosis , which involves granulomas and inflammation of blood vessels ( vasculitis ) , and we cured it with two mg aspirin .')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}