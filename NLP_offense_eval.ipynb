{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "NLP - offense eval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ollema/nlp_offenseeval/blob/master/NLP_offense_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHgz_RkmCEuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "%tensorflow_version 1.x\n",
        "!pip install pytorch-crf\n",
        "!pip install transformers\n",
        "!pip install wordsegment\n",
        "!pip install emoji\n",
        "!pip install tdqm\n",
        "\n",
        "!wget -c https://raw.githubusercontent.com/ollema/nlp_offenseeval/master/OLIDv1.0/olid-training-v1.0.tsv\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchtext\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
        "from transformers import AdamW\n",
        "\n",
        "import emoji\n",
        "from wordsegment import load, segment\n",
        "load()\n",
        "\n",
        "from tqdm import tqdm\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO4i3rrqCEuY",
        "colab_type": "text"
      },
      "source": [
        "## 1. Reading the data\n",
        "\n",
        "<!-- The following function reads a file with OLID instances.\n",
        "\n",
        "Each instance contains up to 3 labels each.\n",
        "\n",
        "Here is some example instances:\n",
        "\n",
        "```\n",
        "id\ttweet\tsubtask_a\tsubtask_b\tsubtask_c\n",
        "86426\t@USER She should ask a few native Americans what their take on this is.\tOFF\tUNT\tNULL\n",
        "90194\t@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL\tOFF\tTIN\tIND\n",
        "16820\tAmazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT\tNOT\tNULL\tNULL\n",
        "62688\t\"@USER Someone should'veTaken\"\" this piece of shit to a volcano. ðŸ˜‚\"\"\"\tOFF\tUNT\tNULL\n",
        "43605\t@USER @USER Obama wanted liberals &amp; illegals to move into red states\tNOT\tNULL\tNULL\n",
        "```\n",
        "Instances are included in TSV format as follows:\n",
        "\n",
        "`ID\tINSTANCE\tSUBA\tSUBB\tSUBC `\n",
        "\n",
        "Whenever a label is not given, a value `NULL` is inserted (e.g. `INSTANCE\tNOT\tNULL\tNULL`) -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_KRR3BtCEua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def desegmentize_hashtags_in_tweet(tweet):\n",
        "    new_tweet = []\n",
        "    for word in tweet.split():\n",
        "        new_word = word\n",
        "        if word[0] == '#':\n",
        "            new_word = \" \".join(segment(word[1:]))\n",
        "        new_tweet.append(new_word)\n",
        "    return \" \".join(new_tweet)\n",
        "\n",
        "def limit_users_in_tweet(tweet):\n",
        "    new_tweet = []\n",
        "    user_count = 0\n",
        "    for word in tweet.split():\n",
        "        if word == \"@USER\":\n",
        "            user_count += 1\n",
        "        else:\n",
        "            user_count = 0\n",
        "        if user_count <= 3:\n",
        "            new_tweet.append(word)\n",
        "    return \" \".join(new_tweet)\n",
        "\n",
        "def read_data(corpus_file, datafields, tokenizer, max_len):\n",
        "    print(f'Reading sentences from {corpus_file}...')\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    with open(corpus_file, encoding='utf-8') as f:\n",
        "        next(f) # skip header line\n",
        "        \n",
        "        n_truncated = 0\n",
        "        examples = []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            _, tweet, label, _, _ = line.split(\"\\t\")\n",
        "\n",
        "            # desegmentize hashtags in tweet\n",
        "            tweet = desegmentize_hashtags_in_tweet(tweet)\n",
        "\n",
        "            # demojize tweet\n",
        "            tweet = emoji.demojize(tweet).replace(\":\", \" \").replace(\"_\", \" \")\n",
        "\n",
        "            # replace URL with http\n",
        "            tweet = tweet.replace(\"URL\", \"http\")\n",
        "\n",
        "            # limit the amount of consecutive @USERs in a tweet\n",
        "            if tweet.count(\"@USER\") > 3:\n",
        "                tweet = limit_users_in_tweet(tweet)\n",
        "\n",
        "            tokens = tokenizer.tokenize(tweet)\n",
        "            \n",
        "            # we need to truncate the sentences\n",
        "            if len(tokens) > max_len-2:\n",
        "                tokens = tokens[:max_len-2]\n",
        "                n_truncated += 1\n",
        "\n",
        "            tweet = \" \".join(tokens)\n",
        "            examples.append(torchtext.data.Example.fromlist([tweet, label], datafields))\n",
        "        \n",
        "        print(f'Read {len(examples)} sentences, truncated {n_truncated}.')\n",
        "        return torchtext.data.Dataset(examples, datafields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnwFBnlMCEvD",
        "colab_type": "text"
      },
      "source": [
        "## 2. Training the classifier\n",
        "<!-- Note that the `train` method returns the best F1-score seen when evaluating on the validation set.\n",
        "\n",
        "The `classify` method will be used in the interactive demo. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IczR7AqyCEvF",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_validation(scores, gold):\n",
        "    \"\"\" gold contains labels 0 for class one and 1 for class 2\"\"\"\n",
        "    guesses = scores.argmax(dim=1)\n",
        "\n",
        "    class_1_correct = ((guesses == 0)*(gold == 0)).sum().item()\n",
        "    class_2_correct = ((guesses == 1)*(gold == 1)).sum().item()\n",
        "\n",
        "    class_1_instances = (gold == 0).sum().item()\n",
        "    class_2_instances = (gold == 1).sum().item()\n",
        "\n",
        "    stats = Counter({'class_1_correct': class_1_correct, 'class_2_correct': class_2_correct,\n",
        "                     'class_1_instances': class_1_instances, 'class_2_instances': class_2_instances})\n",
        "    \n",
        "    n_correct = (guesses == gold).sum().item()\n",
        "\n",
        "    return n_correct, stats\n",
        "\n",
        "\n",
        "class Classifier:\n",
        "    def __init__(self, bert_model_name, distil, max_len, batch_size, epochs):\n",
        "        self.device = 'cuda'\n",
        "        self.distil = distil\n",
        "        \n",
        "        if distil:\n",
        "            self.bert_model_name = \"distil\" + bert_model_name\n",
        "            self.tokenizer = DistilBertTokenizer.from_pretrained(self.bert_model_name, do_lower_case=True)\n",
        "            self.model = DistilBertForSequenceClassification.from_pretrained(self.bert_model_name, num_labels=2)\n",
        "            self.model.to(self.device)\n",
        "        else:\n",
        "            self.bert_model_name = bert_model_name\n",
        "            self.tokenizer = BertTokenizer.from_pretrained(self.bert_model_name, do_lower_case=True)\n",
        "            self.model = BertForSequenceClassification.from_pretrained(self.bert_model_name, num_labels=2)\n",
        "            self.model.to(self.device)\n",
        "\n",
        "        print(f\"Using {self.bert_model_name}\")\n",
        "\n",
        "        self.max_len = max_len\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "            \n",
        "        self.WORD = torchtext.data.Field(\n",
        "            sequential=True, \n",
        "            tokenize=self.tokenizer.tokenize,\n",
        "            pad_token=self.tokenizer.pad_token,\n",
        "            init_token=self.tokenizer.cls_token,\n",
        "            eos_token=self.tokenizer.sep_token\n",
        "        )\n",
        "        self.LABEL = torchtext.data.LabelField(is_target=True)\n",
        "        self.fields = [('tweet', self.WORD), ('label', self.LABEL)]     \n",
        "        \n",
        "                \n",
        "    def train(self):\n",
        "        print('Reading data...')\n",
        "        dataset = read_data(\"olid-training-v1.0.tsv\", self.fields, self.tokenizer, self.max_len) \n",
        "        train, valid = dataset.split([0.8, 0.2])\n",
        "\n",
        "        self.LABEL.build_vocab(train)\n",
        "        self.WORD.build_vocab(train)\n",
        "        self.WORD.vocab.stoi = self.tokenizer.vocab\n",
        "        self.WORD.vocab.itos = list(self.tokenizer.vocab)\n",
        "        \n",
        "        train_iterator = torchtext.data.BucketIterator(\n",
        "            train,\n",
        "            device=self.device,\n",
        "            batch_size=self.batch_size,\n",
        "            sort_key=lambda x: len(x.tweet),\n",
        "            repeat=False,\n",
        "            train=True,\n",
        "            sort=True)\n",
        "\n",
        "        valid_iterator = torchtext.data.BucketIterator(\n",
        "            valid,\n",
        "            device=self.device,\n",
        "            batch_size=self.batch_size,\n",
        "            sort_key=lambda x: len(x.tweet),\n",
        "            repeat=False,\n",
        "            train=False,\n",
        "            sort=True)\n",
        "        \n",
        "        train_batches = list(train_iterator)\n",
        "        valid_batches = list(valid_iterator)\n",
        "        \n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        decay = 0.01\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': decay},\n",
        "            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "            \n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8)\n",
        "        \n",
        "        history = defaultdict(list)    \n",
        "        \n",
        "        for epoch in range(1, self.epochs + 1):\n",
        "\n",
        "            t0 = time.time()\n",
        "        \n",
        "            loss_sum = 0\n",
        "            n_batches = 0\n",
        "\n",
        "            self.model.train()\n",
        "            train_fmt = \"{desc}:   {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}{postfix}]\"\n",
        "            for i, batch in enumerate(tqdm(train_iterator, desc=\"Training\", bar_format=train_fmt)):\n",
        "\n",
        "                tweets = batch.tweet.t()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = self.model(tweets, labels=batch.label)\n",
        "                            \n",
        "                loss = outputs[0]\n",
        "                \n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                            \n",
        "                loss_sum += loss.item()\n",
        "                n_batches += 1\n",
        "            \n",
        "            train_loss = loss_sum / n_batches\n",
        "            history['train_loss'].append(train_loss)\n",
        "            \n",
        "            n_correct = 0\n",
        "            stats = Counter({'class_1_correct': 0, 'class_2_correct': 0,\n",
        "             'class_1_instances': 0, 'class_2_instances': 0})\n",
        "            n_valid = len(valid)\n",
        "            loss_sum = 0\n",
        "            n_batches = 0\n",
        "\n",
        "            self.model.eval()\n",
        "            valid_fmt = \"{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}{postfix}]\"\n",
        "            for i, batch in enumerate(tqdm(valid_iterator, desc=\"Validating\", bar_format=valid_fmt)):\n",
        "                tweets = batch.tweet.t()\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    outputs = self.model(tweets, labels=batch.label)\n",
        "                    loss_batch, scores = outputs\n",
        "                    \n",
        "                loss_sum += loss_batch.item()\n",
        "\n",
        "                new_correct, new_stats = evaluate_validation(scores, batch.label)\n",
        "                stats += new_stats\n",
        "                n_correct += new_correct\n",
        "                n_batches += 1\n",
        "\n",
        "            macro_F1_score = (stats['class_1_correct']/stats['class_1_instances'] + stats['class_2_correct']/stats['class_2_instances'])/2\n",
        "            val_acc = n_correct / n_valid\n",
        "            val_loss = loss_sum / n_batches\n",
        "            \n",
        "            history['F1_score'].append(macro_F1_score)\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['val_acc'].append(val_acc)   \n",
        "                    \n",
        "            t1 = time.time()\n",
        "            print()\n",
        "            print(f'Epoch {epoch}: train loss = {train_loss:.4f}, val loss = {val_loss:.4f}, val acc: {val_acc:.4f}, time = {t1-t0:.4f}\\n')\n",
        "        \n",
        "        return history[\"val_acc\"][-1], history['F1_score'][-1]\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTYRNJBMCEvO",
        "colab_type": "code",
        "outputId": "70273319-ba42-4200-8299-3d6b67dd53a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "f_scores = []\n",
        "\n",
        "for i in range(1):\n",
        "    torch.manual_seed(i * 42) and random.seed(i * 42) and torch.cuda.manual_seed_all(i * 42)\n",
        "\n",
        "    classifier = Classifier(\n",
        "        bert_model_name=\"bert-base-uncased\",\n",
        "        distil=False,\n",
        "        max_len=120,\n",
        "        batch_size=32,\n",
        "        epochs=1\n",
        "    )\n",
        "\n",
        "    accuracy, f_score = classifier.train()\n",
        "    f_scores.append(f_score)\n",
        "\n",
        "print(f\"mean f-score: {np.mean(f_scores)}\")\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using bert-base-uncased\n",
            "Reading data...\n",
            "Reading sentences from olid-training-v1.0.tsv...\n",
            "Read 13240 sentences, truncated 1.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Training:   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 331/331 [02:44]\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83/83 [00:13]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: train loss = 0.4819, val loss = 0.4755, val acc: 0.7870, time = 177.8853\n",
            "\n",
            "mean f-score: 0.7738019498831682\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}