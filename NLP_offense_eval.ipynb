{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "NLP - offense eval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ollema/nlp_offenseeval/blob/master/NLP_offense_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSd4_idAEImt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "5155f837-b0ba-4f89-b941-04e0c3b8e97b"
      },
      "source": [
        "!pip install pytorch-crf\n",
        "!pip install transformers\n",
        "!pip install wordsegment\n",
        "!pip install emoji\n",
        "\n",
        "!wget -c https://raw.githubusercontent.com/ollema/nlp_offenseeval/master/OLIDv1.0/olid-training-v1.0.tsv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.6/dist-packages (0.7.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.36)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.83)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.35)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.36 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.36)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: wordsegment in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "--2019-12-16 16:31:10--  https://raw.githubusercontent.com/ollema/nlp_offenseeval/master/OLIDv1.0/olid-training-v1.0.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHgz_RkmCEuO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "233211cf-d486-4ffe-db0b-746bd9e378aa"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import time\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "import random\n",
        "\n",
        "import emoji\n",
        "from wordsegment import load, segment\n",
        "load()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, BertForTokenClassification, AdamW\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina' \n",
        "plt.style.use('seaborn')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO4i3rrqCEuY",
        "colab_type": "text"
      },
      "source": [
        "## 1. Reading the data\n",
        "\n",
        "The following function reads a file with OLID instances.\n",
        "\n",
        "Each instance contains up to 3 labels each.\n",
        "\n",
        "Here is some example instances:\n",
        "\n",
        "```\n",
        "id\ttweet\tsubtask_a\tsubtask_b\tsubtask_c\n",
        "86426\t@USER She should ask a few native Americans what their take on this is.\tOFF\tUNT\tNULL\n",
        "90194\t@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL\tOFF\tTIN\tIND\n",
        "16820\tAmazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT\tNOT\tNULL\tNULL\n",
        "62688\t\"@USER Someone should'veTaken\"\" this piece of shit to a volcano. ðŸ˜‚\"\"\"\tOFF\tUNT\tNULL\n",
        "43605\t@USER @USER Obama wanted liberals &amp; illegals to move into red states\tNOT\tNULL\tNULL\n",
        "```\n",
        "Instances are included in TSV format as follows:\n",
        "\n",
        "`ID\tINSTANCE\tSUBA\tSUBB\tSUBC `\n",
        "\n",
        "Whenever a label is not given, a value `NULL` is inserted (e.g. `INSTANCE\tNOT\tNULL\tNULL`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_KRR3BtCEua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def desegmentize_hashtags_in_tweet(tweet):\n",
        "    new_tweet = []\n",
        "    for word in tweet.split():\n",
        "        new_word = word\n",
        "        if word[0] == '#':\n",
        "            new_word = \" \".join(segment(word[1:]))\n",
        "        new_tweet.append(new_word)\n",
        "    return \" \".join(new_tweet)\n",
        "\n",
        "def limit_users_in_tweet(tweet):\n",
        "    new_tweet = []\n",
        "    user_count = 0\n",
        "    for word in tweet.split():\n",
        "        if word == \"@USER\":\n",
        "            user_count += 1\n",
        "        else:\n",
        "            user_count = 0\n",
        "        if user_count <= 3:\n",
        "            new_tweet.append(word)\n",
        "    return \" \".join(new_tweet)\n",
        "\n",
        "def read_data(corpus_file, datafields, tokenizer, max_len):\n",
        "    print(f'Reading sentences from {corpus_file}...')\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    with open(corpus_file, encoding='utf-8') as f:\n",
        "        next(f) # skip header line\n",
        "        \n",
        "        n_truncated = 0\n",
        "        examples = []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            _, tweet, label, _, _ = line.split(\"\\t\")\n",
        "\n",
        "            # desegmentize hashtags in tweet\n",
        "            tweet = desegmentize_hashtags_in_tweet(tweet)\n",
        "\n",
        "            # demojize tweet\n",
        "            tweet = emoji.demojize(tweet).replace(\":\", \" \").replace(\"_\", \" \")\n",
        "\n",
        "            # replace URL with http\n",
        "            tweet = tweet.replace(\"URL\", \"http\")\n",
        "\n",
        "            # limit the amount of consecutive @USERs in a tweet\n",
        "            if tweet.count(\"@USER\") > 3:\n",
        "                tweet = limit_users_in_tweet(tweet)\n",
        "\n",
        "            tokens = tokenizer.tokenize(tweet)\n",
        "            \n",
        "            # we need to truncate the sentences\n",
        "            if len(tokens) > max_len-2:\n",
        "                tokens = tokens[:max_len-2]\n",
        "                n_truncated += 1\n",
        "\n",
        "            tweet = \" \".join(tokens)\n",
        "            examples.append(torchtext.data.Example.fromlist([tweet, label], datafields))\n",
        "        \n",
        "        print(f'Read {len(examples)} sentences, truncated {n_truncated}.')\n",
        "        return torchtext.data.Dataset(examples, datafields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxRgYPC7CEuf",
        "colab_type": "text"
      },
      "source": [
        "## 2. The sentence encoder\n",
        "\n",
        "This is the part that will requite a few small modifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lOTERZegIin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertSentenceEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, word_field, bert_model_name, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bert_model_name = bert_model_name\n",
        "        self.model = BertModel.from_pretrained(bert_model_name)\n",
        "        self.output_size = 768\n",
        "\n",
        "    def forward(self, words, chars):\n",
        "        outputs = self.model(words)\n",
        "        last_hidden_states = outputs[0]\n",
        "\n",
        "        return last_hidden_states\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sZcqiByCEu-",
        "colab_type": "text"
      },
      "source": [
        "## 5. Evaluating the predicted named entities\n",
        "\n",
        "The evaluation code is identical to that used in Lecture 6.\n",
        "\n",
        "To evaluate our named entity recognizers, we compare the named entities predicted by the system to the entities in the gold standard. We follow standard practice and compute [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) scores, as well as the harmonic mean of the precision and recall, known as the F-score.\n",
        "\n",
        "Please note that the precision and recall scores are computed with respect to the full named entity spans and labels. To be counted as a correct prediction, the system needs to predict all words in the named entity correctly, and assign the right type of entity label. We don't give any credits to partially correct predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jFg2tlNCEu_",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Convert a list of BIO labels, coded as integers, into spans identified by a beginning, an end, and a label.\n",
        "# To allow easy comparison later, we store them in a dictionary indexed by the start position.\n",
        "def to_spans(l_ids, voc):\n",
        "    spans = {}\n",
        "    current_lbl = None\n",
        "    current_start = None\n",
        "    for i, l_id in enumerate(l_ids):\n",
        "        l = voc[l_id]\n",
        "\n",
        "        if l[0] == 'B': \n",
        "            # Beginning of a named entity: B-something.\n",
        "            if current_lbl:\n",
        "                # If we're working on an entity, close it.\n",
        "                spans[current_start] = (current_lbl, i)\n",
        "            # Create a new entity that starts here.\n",
        "            current_lbl = l[2:]\n",
        "            current_start = i\n",
        "        elif l[0] == 'I':\n",
        "            # Continuation of an entity: I-something.\n",
        "            if current_lbl:\n",
        "                # If we have an open entity, but its label does not\n",
        "                # correspond to the predicted I-tag, then we close\n",
        "                # the open entity and create a new one.\n",
        "                if current_lbl != l[2:]:\n",
        "                    spans[current_start] = (current_lbl, i)\n",
        "                    current_lbl = l[2:]\n",
        "                    current_start = i\n",
        "            else:\n",
        "                # If we don't have an open entity but predict an I tag,\n",
        "                # we create a new entity starting here even though we're\n",
        "                # not following the format strictly.\n",
        "                current_lbl = l[2:]\n",
        "                current_start = i\n",
        "        else:\n",
        "            # Outside: O.\n",
        "            if current_lbl:\n",
        "                # If we have an open entity, we close it.\n",
        "                spans[current_start] = (current_lbl, i)\n",
        "                current_lbl = None\n",
        "                current_start = None\n",
        "    return spans\n",
        "\n",
        "# Compares two sets of spans and records the results for future aggregation.\n",
        "def compare(gold, pred, stats):\n",
        "    for start, (lbl, end) in gold.items():\n",
        "        stats['total']['gold'] += 1\n",
        "        stats[lbl]['gold'] += 1\n",
        "    for start, (lbl, end) in pred.items():\n",
        "        stats['total']['pred'] += 1\n",
        "        stats[lbl]['pred'] += 1\n",
        "    for start, (glbl, gend) in gold.items():\n",
        "        if start in pred:\n",
        "            plbl, pend = pred[start]\n",
        "            if glbl == plbl and gend == pend:\n",
        "                stats['total']['corr'] += 1\n",
        "                stats[glbl]['corr'] += 1\n",
        "\n",
        "# This function combines the auxiliary functions we defined above.\n",
        "def evaluate_iob(predicted, gold, label_field, stats):\n",
        "    # The gold-standard labels are assumed to be an integer tensor of shape\n",
        "    # (max_len, n_sentences), as returned by torchtext.\n",
        "    gold_cpu = gold.cpu().numpy()\n",
        "    gold_cpu = list(gold_cpu.reshape(-1))\n",
        "\n",
        "    # The predicted labels assume the format produced by pytorch-crf, so we\n",
        "    # assume that they have been converted into a list already.\n",
        "    # We just flatten the list.\n",
        "    pred_cpu = [l for sen in predicted for l in sen]\n",
        "    \n",
        "    # Compute spans for the gold standard and prediction.\n",
        "    gold_spans = to_spans(gold_cpu, label_field.vocab.itos)\n",
        "    pred_spans = to_spans(pred_cpu, label_field.vocab.itos)\n",
        "\n",
        "    # Finally, update the counts for correct, predicted and gold-standard spans.\n",
        "    compare(gold_spans, pred_spans, stats)\n",
        "\n",
        "# Computes precision, recall and F-score, given a dictionary that contains\n",
        "# the counts of correct, predicted and gold-standard items.\n",
        "def prf(stats):\n",
        "    if stats['pred'] == 0:\n",
        "        return 0, 0, 0\n",
        "    p = stats['corr']/stats['pred']\n",
        "    r = stats['corr']/stats['gold']\n",
        "    if p > 0 and r > 0:\n",
        "        f = 2*p*r/(p+r)\n",
        "    else:\n",
        "        f = 0\n",
        "    return p, r, f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnwFBnlMCEvD",
        "colab_type": "text"
      },
      "source": [
        "## 6. Training the sequence tagger\n",
        "\n",
        "Finally, the main class `Tagger`, which combines all the pieces defined above. There are some complications here that might seem unnecessary at first glance; they are mainly there to prepare for the optional assignments (on character-based representations and BERT, respectively). Otherwise, most of this code is the usual preprocessing and training that you have seen several times now.\n",
        "\n",
        "Note that the `train` method returns the best F1-score seen when evaluating on the validation set.\n",
        "\n",
        "The `tag` method will be used in the interactive demo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IczR7AqyCEvF",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "class Classifier:\n",
        "    def __init__(self, config, gensim_model=None, bert_model_name=None):\n",
        "        self.config = config\n",
        "        self.bert_model_name = bert_model_name\n",
        "        lowercase = 'uncased' in bert_model_name\n",
        "        print('Lowercased BERT model?', lowercase)\n",
        "        \n",
        "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=lowercase)\n",
        "        pad = self.tokenizer.pad_token           \n",
        "        self.WORD = torchtext.data.Field(init_token=self.tokenizer.cls_token, eos_token=self.tokenizer.sep_token, sequential=True, lower=lowercase, pad_token=pad, batch_first=True)\n",
        "        self.LABEL = torchtext.data.Field(is_target=True, init_token='O', eos_token=pad, pad_token=pad, sequential=True, unk_token=None, batch_first=True)\n",
        "        self.fields = [('words', self.WORD), ('labels', self.LABEL)]     \n",
        "        self.device = 'cuda'\n",
        "                \n",
        "    def train(self):\n",
        "        print('Reading and tokenizing...')\n",
        "        dataset = read_data(self.config.dataset, self.fields, self.tokenizer, 128) \n",
        "        train, valid = dataset.split([0.8, 0.2])\n",
        "\n",
        "        self.LABEL.build_vocab(train)\n",
        "        self.WORD.build_vocab(train)\n",
        "        # Here, we tell torchtext to use the vocabulary of BERT's tokenizer.\n",
        "        # .stoi is the map from strings to integers, and itos from integers to strings.\n",
        "        self.WORD.vocab.stoi = self.tokenizer.vocab\n",
        "        self.WORD.vocab.itos = list(self.tokenizer.vocab)\n",
        "\n",
        "        n_labels = len(self.LABEL.vocab)\n",
        "        print(f\"n_labels: {n_labels}\")\n",
        "        print(self.LABEL.vocab.freqs)\n",
        "        print(f\"Using BertForSequenceClassification\")\n",
        "        self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=n_labels)    \n",
        "        self.model.to(self.device)\n",
        "\n",
        "        raise RuntimeError\n",
        "            \n",
        "        train_iterator = torchtext.data.BucketIterator(\n",
        "            train,\n",
        "            device=self.device,\n",
        "            batch_size=self.config.train_batch_size,\n",
        "            sort_key=lambda x: len(x.words),\n",
        "            repeat=False,\n",
        "            train=True,\n",
        "            sort=True)\n",
        "\n",
        "        valid_iterator = torchtext.data.BucketIterator(\n",
        "            valid,\n",
        "            device=self.device,\n",
        "            batch_size=self.config.valid_batch_size,\n",
        "            sort_key=lambda x: len(x.words),\n",
        "            repeat=False,\n",
        "            train=False,\n",
        "            sort=True)\n",
        "        \n",
        "        # train_iterator = torchtext.data.Iterator(\n",
        "        #     train,\n",
        "        #     device=device,\n",
        "        #     batch_size=32,\n",
        "        #     repeat=False,\n",
        "        #     train=True,\n",
        "        #     sort=False)\n",
        "\n",
        "        # valid_iterator = torchtext.data.Iterator(\n",
        "        #     valid,\n",
        "        #     device=device,\n",
        "        #     batch_size=32,\n",
        "        #     repeat=False,\n",
        "        #     train=False,\n",
        "        #     sort=False)\n",
        "        \n",
        "        train_batches = list(train_iterator)\n",
        "        valid_batches = list(valid_iterator)\n",
        "        \n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        decay = 0.01\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': decay},\n",
        "            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "            \n",
        "        # As discussed above, we use the AdamW optimizer from the transformers library. It seems to\n",
        "        # give slightly better results than the standard Adam.\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8)\n",
        "        \n",
        "        history = defaultdict(list)    \n",
        "        best_f1 = -1\n",
        "        \n",
        "        for i in range(1, self.config.n_epochs + 1):\n",
        "\n",
        "            t0 = time.time()\n",
        "\n",
        "            loss_sum = 0\n",
        "\n",
        "            random.shuffle(train_batches)\n",
        "            \n",
        "            self.model.train()\n",
        "            for batch in train_batches:\n",
        "                if verbose:\n",
        "                    print('.', end='')\n",
        "                    sys.stdout.flush()\n",
        "                \n",
        "                chars = batch.chars if self.config.use_characters else None\n",
        "                loss = self.model(batch.words, chars, batch.labels)\n",
        "                \n",
        "                optimizer.zero_grad()            \n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                loss_sum += loss.item()\n",
        "\n",
        "            train_loss = loss_sum / len(train_batches)\n",
        "            history['train_loss'].append(train_loss)\n",
        "\n",
        "            if verbose:\n",
        "                print()\n",
        "            \n",
        "            # Evaluate on the validation set.\n",
        "            stats = defaultdict(Counter)\n",
        "\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                for batch in valid_batches:\n",
        "                    if verbose:\n",
        "                        print('.', end='')\n",
        "                        sys.stdout.flush()                        \n",
        "                    # Predict the model's output on a batch.\n",
        "                    chars = batch.chars if self.config.use_characters else None\n",
        "                    predicted = self.model.predict(batch.words, chars)\n",
        "\n",
        "                    # Update the evaluation statistics.\n",
        "                    evaluate_iob(predicted, batch.labels, self.LABEL, stats)\n",
        "\n",
        "            if verbose:\n",
        "                print()                \n",
        "\n",
        "            # Compute the overall F-score for the validation set.\n",
        "            _, _, val_f1 = prf(stats['total'])\n",
        "\n",
        "            if val_f1 > best_f1:\n",
        "                best_f1 = val_f1\n",
        "                best_epoch = i\n",
        "                best_stats = stats\n",
        "                \n",
        "            history['val_f1'].append(val_f1)\n",
        "\n",
        "            t1 = time.time()\n",
        "            if verbose or (i % 5 == 0):\n",
        "                print(f'Epoch {i}: train loss = {train_loss:.4f}, val f1: {val_f1:.4f}, time = {t1-t0:.4f}')\n",
        "           \n",
        "        # After the final evaluation, we print more detailed evaluation statistics, including\n",
        "        # precision, recall, and F-scores for the different types of named entities.\n",
        "        print()\n",
        "        print(f'Best result on the validation set (epoch {best_epoch}):')\n",
        "        p, r, f1 = prf(best_stats['total'])\n",
        "        print(f'Overall: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')\n",
        "        for label in stats:\n",
        "            if label != 'total':\n",
        "                p, r, f1 = prf(best_stats[label])\n",
        "                print(f'{label:4s}: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')\n",
        "        \n",
        "        plt.plot(history['train_loss'])\n",
        "        plt.plot(history['val_f1'])\n",
        "        plt.legend(['training loss', 'validation F-score'])\n",
        "        return best_f1\n",
        "        \n",
        "    # def tag(self, sentences):\n",
        "    #     # This method applies the trained model to a list of sentences.\n",
        "        \n",
        "    #     # First, create a torchtext Dataset containing the sentences to tag.\n",
        "    #     examples = []\n",
        "    #     for sen in sentences:\n",
        "    #         examples.append(torchtext.data.Example.fromlist([sen, []], self.fields))\n",
        "    #     dataset = torchtext.data.Dataset(examples, self.fields)\n",
        "        \n",
        "    #     iterator = torchtext.data.Iterator(\n",
        "    #         dataset,\n",
        "    #         device=self.device,\n",
        "    #         batch_size=len(examples),\n",
        "    #         repeat=False,\n",
        "    #         train=False,\n",
        "    #         sort=False)\n",
        "        \n",
        "    #     # Apply the trained model to the batch.\n",
        "    #     self.model.eval()\n",
        "    #     with torch.no_grad():\n",
        "    #         for batch in iterator:\n",
        "    #             # Call the model's predict method. This returns a list of NumPy matrix\n",
        "    #             # containing the integer-encoded tags for each sentence.\n",
        "\n",
        "    #             chars = batch.chars if self.config.use_characters else None\n",
        "    #             predicted = self.model.predict(batch.words, chars)\n",
        "\n",
        "    #             # Convert the integer-encoded tags to tag strings.\n",
        "    #             out = []\n",
        "    #             for tokens, pred_sen in zip(sentences, predicted):\n",
        "    #                 out.append([self.LABEL.vocab.itos[pred_id] for _, pred_id in zip(tokens, pred_sen[1:])])\n",
        "    #             return out\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I5iuwKXCEvI",
        "colab_type": "text"
      },
      "source": [
        "The `TaggerConfig` bundles the various configuration options into a single container."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9QksBhnCEvK",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "class ClassifierConfig(object):\n",
        "    \n",
        "    # Location of training and validation data.\n",
        "    dataset = 'olid-training-v1.0.tsv'\n",
        "    \n",
        "    # Batch size for the training and validation set.\n",
        "    train_batch_size = 32\n",
        "    valid_batch_size = 64\n",
        "    \n",
        "    # Number of training epochs.\n",
        "    n_epochs=2\n",
        "    \n",
        "    # Word dropout probability.\n",
        "    word_dropout_prob = 0.2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTYRNJBMCEvO",
        "colab_type": "code",
        "outputId": "d4419692-956d-43a6-84d1-9a01e465fff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        }
      },
      "source": [
        "f_scores = []\n",
        "\n",
        "for i in range(1):\n",
        "    torch.manual_seed(i * 1000) and random.seed(i * 1000)\n",
        "\n",
        "    classifier = Classifier(config=ClassifierConfig(), bert_model_name=\"bert-base-uncased\")\n",
        "\n",
        "    f_scores.append(classifier.train())\n",
        "\n",
        "print(f\"mean f-score: {np.mean(f_scores)}\")\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lowercased BERT model? True\n",
            "Reading and tokenizing...\n",
            "Reading sentences from olid-training-v1.0.tsv...\n",
            "Read 13240 sentences, truncated 1.\n",
            "n_labels: 4\n",
            "Counter({'NOT': 7067, 'OFF': 3525})\n",
            "Using BertForSequenceClassification\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-9299ae648602>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mClassifierConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mf_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"mean f-score: {np.mean(f_scores)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-47e7aa09817f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLABEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Using BertForSequenceClassification\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BertForSequenceClassification' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFq5uEwjCEvS",
        "colab_type": "text"
      },
      "source": [
        "## 7. Running the sequence tagger\n",
        "\n",
        "We create a utility function that applies the tagger a given sentence, and then shows the sentence with the diseases and chemicals highlighted in red and blue, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOwfaJ1gCEvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "def show_entities(sentence):\n",
        "    if tagger.tokenizer:\n",
        "        tokens = tagger.tokenizer.tokenize(sentence)\n",
        "    else:\n",
        "        tokens = sentence.split()\n",
        "    tags = tagger.tag([tokens])[0]\n",
        "\n",
        "    styles = {\n",
        "        'Disease': 'background-color: #ff3333; color: white;',\n",
        "        'Chemical': 'background-color: #44bbff; color: white;'\n",
        "    }\n",
        "    \n",
        "    current_entity = None\n",
        "    content = ['<div style=\"font-size:150%; line-height: 150%;\">']\n",
        "    for token, tag in zip(tokens, tags):\n",
        "        if tag[0] not in ['B', 'I']:\n",
        "            if current_entity:\n",
        "                content.append('</b>')\n",
        "                current_entity = None\n",
        "            content.append(' ')\n",
        "        elif tag[0] == 'B':\n",
        "            if current_entity:\n",
        "                content.append('</b>')\n",
        "            content.append(' ')\n",
        "            current_entity = tag[2:]\n",
        "            content.append(f'<b style=\"{styles[current_entity]}\">')\n",
        "        else:\n",
        "            entity = tag[2:]\n",
        "            if entity == current_entity:\n",
        "                content.append(' ')\n",
        "            elif current_entity is None:\n",
        "                content.append(' ')\n",
        "                content.append(f'<b style=\"{styles[entity]}\">')\n",
        "            else:\n",
        "                content.append('</b>')\n",
        "                content.append(' ')\n",
        "                content.append(f'<b style=\"{styles[entity]}\">')\n",
        "            current_entity = entity\n",
        "        content.append(token)\n",
        "    if current_entity:\n",
        "        content.append('</b>')\n",
        "    content.append('</div>')\n",
        "    \n",
        "    html = ''.join(content).replace(\" ##\", \"\").strip()\n",
        "    display(HTML(html))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA7ENb72CEvX",
        "colab_type": "text"
      },
      "source": [
        "And here are some examples, some invented and some taken from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkIvN22SCEvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_entities('Severe arrythmia cured with aspirin and oxycontin pills .')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_iMhqEeCEvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_entities('In conclusion, hyperammonemic encephalopathy can occur in patienst receiving continuous infusion of 5 - FU .')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjhX66wLCEvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_entities('The authors describe the case of a 56 - year - old woman with chronic , severe heart failure secondary to dilated cardiomyopathy and absence of significant ventricular arrhythmias who developed bubonic plague and AIDS and torsade de pointes ventricular tachycardia during one cycle of intermittent low dose ( 2 . 5 mcg / kg per min ) aspirin .')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzO8StnvCEvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_entities('She had heart failure , bubonic plague , AIDS and ventricular tachycardia so we had to give her some aspirin and oxycontin .')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzkeNlpsCEvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_entities('A severe case of granulomatosis with polyangiitis , also known as Wegener \\' s granulomatosis , which involves granulomas and inflammation of blood vessels ( vasculitis ) , and we cured it with two mg aspirin .')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}